{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colon glands segmentation with U-Net\n",
    "<img src=\"images/glas_header.bmp\" style=\"width:100%;\" height=\"250\" align=\"center\">\n",
    "\n",
    "In this assignment we are going to develop **U-Net** models to segment benign and malignant glands in histopathology images of colon tissue, stained with hematoxylin and eosin (H&E).\n",
    "\n",
    "## Teaching assistants\n",
    "\n",
    "- Hans Pinckaers: Hans.Pinckaers@radboudumc.nl \n",
    "- Thomas De Bel: Thomas.deBel@radboudumc.nl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "Please fill in this cell with your name and e-mail address. This information will be used to grade your assignment.\n",
    "\n",
    "* Name student #1, email address: ...\n",
    "* Name student #2, email address: ...\n",
    "* Name student #3, email address: ...\n",
    "\n",
    "Please submit your notebook via grand-challenge.org: https://ismi-glas.grand-challenge.org/\n",
    "\n",
    "This notebook requires tensorflow and keras installed, please make sure you installed the required packages: https://ismi-nodules.grand-challenge.org/Resources/\n",
    "\n",
    "Submit a notebook **WITH ALL CELLS EXECUTED!!!**\n",
    "\n",
    "* Groups: You should work in pairs or alone. Working in groups of 2-3 is preferable.\n",
    "* Deadline for this assignment: \n",
    " * Friday, March 22nd until 23:59h.\n",
    " * 5 points (maximum grade = 100 points) penalization per day after deadline.\n",
    "* Submit your **fully executed** notebook to the grand-challenge.org platform ** AND MAIL THE SOLUTION TO THE STUDENT ASSISTANTS**\n",
    "* The file name of the notebook you submit **must be** ```NameSurname1_NameSurname2_NameSurname3.ipynb```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "<img src=\"images/instance_segmentation.png\" width=\"500\" height=\"250\" align=\"right\">\n",
    "For this assignment we will use data from the publicly available GlaS dataset (https://warwick.ac.uk/fac/sci/dcs/research/tia/glascontest/).\n",
    "The original GlaS dataset consists of 165 images, 85 used for training and 80 used for testing. Each case contains:\n",
    "* a small crop of histopathology (RGB) image\n",
    "* manual annotations of colon glands, both benign and malignant\n",
    "\n",
    "Similar to what done in the original U-Net paper, the main task of the GlaS challenge was **instance segmentation**, meaning that participants in the challenge had to build a model that was good at separating glands from background (segmentation) but also at identifying/detecting glands. Therefore, the type of manual annotations that come with the original GlaS dataset is similar to what shown in the image on the right in this cell. You can clearly see that each gland in the manually annotated mask gets a different label. Additionally, the size of images in the original dataset is not fixed and is approximately 500 x 700 pixels.\n",
    "\n",
    "### \"Binary GlaS 256\"\n",
    "In this assignment, we have simplified things a bit. In particular:\n",
    "* we have resized all images to 256x256, both in the training and in the test set\n",
    "* we have converted all manual annotations to a binary form, removing the task of instance segmentation\n",
    "\n",
    "Examples of images in the dataset of this assignment (which we can call \"binary glas 256\") are the following:\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td><img src=\"images/1.png\" width=\"120\" height=\"120\" align=\"left\"></td>\n",
    "<td><img src=\"images/1_anno.png\" width=\"120\" height=\"120\" align=\"left\"></td>\n",
    "<td><img src=\"images/2.png\" width=\"120\" height=\"120\" align=\"left\"></td>\n",
    "<td><img src=\"images/2_anno.png\" width=\"120\" height=\"120\" align=\"left\"></td>\n",
    "<td><img src=\"images/3.png\" width=\"120\" height=\"120\" align=\"left\"></td>\n",
    "<td><img src=\"images/3_anno.png\" width=\"120\" height=\"120\" align=\"left\"></td>\n",
    "<td><img src=\"images/4.png\" width=\"120\" height=\"120\" align=\"left\"></td>\n",
    "<td><img src=\"images/4_anno.png\" width=\"120\" height=\"120\" align=\"left\"></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "This means that the datasets you will be downloading in this notebook are a modified version of the original ones, and are only valid for the purpose of this assignment. Also, the results you will be getting in this notebook are not directly applicable to the leaderboard of the official GlaS challenge. Still, the setting we propose is a good playground to experiment with the U-Net model, which is the main goal of this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "The main tasks of this assignment are the following:\n",
    "\n",
    "### 1. Train a baseline U-Net model (5 points)\n",
    "### 2. Implement a basic ``unet_block`` with batch normalization, build and train a new U-Net model (30 points)\n",
    "### 3. Add regularization by adding dropout to the unet_block and an L2 regularization loss (25 points)\n",
    "### 4. Use weight maps (25 points)\n",
    "### 5. Use valid convolutions (35 points)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries\n",
    "import requests\n",
    "from tqdm import trange\n",
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import matplotlib\n",
    "from random import randint\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "from skimage.transform import rescale, resize\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import shutil\n",
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Dropout, UpSampling2D, concatenate, Cropping2D, Reshape, BatchNormalization\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD, Adam\n",
    "import keras.callbacks\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "# this part is needed if you run the notebook on Cartesius with multiple cores\n",
    "n_cores = 32\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=n_cores-1, inter_op_parallelism_threads=1, allow_soft_placement=True)\n",
    "session = tf.Session(config=config)\n",
    "K.set_session(session)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(n_cores-1)\n",
    "os.environ[\"KMP_BLOCKTIME\"] = \"1\"\n",
    "os.environ[\"KMP_SETTINGS\"] = \"1\"\n",
    "os.environ[\"KMP_AFFINITY\"]= \"granularity=fine,verbose,compact,1,0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define your local directory where data is stored\n",
    "data_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download GlaS dataset resized to 256x256 from SURFDrive\n",
    "link = 'https://surfdrive.surf.nl/files/index.php/s/D5TLR0rPaUogqr7/download'\n",
    "file_name = \"glas_256.zip\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in response.iter_content(chunk_size=4096):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "with zipfile.ZipFile(file_name,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(data_dir)\n",
    "os.remove(os.path.join(data_dir, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some classes and functions that will be used throughout this assignment. Some classes, like the ``Dataset`` class, have been used already in previous assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \n",
    "    def __init__(self, imgs, lbls=None):\n",
    "        self.imgs = imgs\n",
    "        self.lbls = lbls\n",
    "    \n",
    "    def get_lenght(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def show_image(self, i):\n",
    "        if self.lbls != None:\n",
    "            f, axes = plt.subplots(1, 2)\n",
    "            for ax, im, t in zip(axes, \n",
    "                                 (self.imgs[i], self.lbls[i]), \n",
    "                                 ('RGB image', \n",
    "                                  'Manual annotation; Range: [{}, {}]'.format(self.lbls[i].min(), \n",
    "                                                                              self.lbls[i].max()))):\n",
    "                ax.imshow(im)\n",
    "                ax.set_title(t)\n",
    "        else:\n",
    "            plt.imshow(self.imgs[i])\n",
    "            plt.title('RGB image')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    return np.array(Image.open(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training purposes, we have to extract patches from the training set to build our model. Therefore, we have to define a patch extractor and a batch generator. Luckily, we can reuse some of the code used in previous assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExtractor:\n",
    "\n",
    "    def __init__(self, patch_size, flipping=True):\n",
    "        self.patch_size = patch_size \n",
    "        self.flipping = flipping\n",
    "    \n",
    "    def get_patch(self, image, label):\n",
    "        ''' \n",
    "        Get a patch of patch_size from input image, along with corresponding label map.\n",
    "        This function works with image size >= patch_size, and pick random location of the patch inside the image.\n",
    "        (Possibly) return a flipped version of the image and corresponding label.\n",
    "\n",
    "        image: a numpy array representing the input image\n",
    "        label: a numpy array representing the labels corresponding to input image\n",
    "        '''\n",
    "        \n",
    "        # pick a random location\n",
    "        dims = image.shape        \n",
    "        r = randint(0, dims[0]-self.patch_size[0])\n",
    "        c = randint(0, dims[1]-self.patch_size[1])\n",
    "        \n",
    "        patch = image[r:r+self.patch_size[0], c:c+self.patch_size[1], :]\n",
    "        target = label[r:r+self.patch_size[0], c:c+self.patch_size[1]].reshape(self.patch_size[0], self.patch_size[1], 1)\n",
    "\n",
    "        patch_out = patch / 255. # normalize image intensity to range [0., 1.]\n",
    "        target_out = target\n",
    "        \n",
    "        # random flipping\n",
    "        flip1 = self.flipping and random.random() > 0.5\n",
    "        flip2 = self.flipping and random.random() > 0.5\n",
    "        if flip1:\n",
    "            patch_out = np.fliplr(patch_out)\n",
    "            target_out = np.fliplr(target_out)\n",
    "        if flip2:\n",
    "            patch_out = np.flipud(patch_out)\n",
    "            target_out = np.flipud(target_out)\n",
    "            \n",
    "        return patch_out, target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCreator:\n",
    "    \n",
    "    def __init__(self, patch_extractor, dataset, target_size):\n",
    "        self.patch_extractor = patch_extractor\n",
    "        self.target_size = target_size # size of the output, can be useful when valid convolutions are used\n",
    "        \n",
    "        self.imgs = dataset.imgs\n",
    "        self.lbls = dataset.lbls\n",
    "                \n",
    "        self.n = len(self.imgs)\n",
    "        self.patch_size = self.patch_extractor.patch_size\n",
    "    \n",
    "    def create_image_batch(self, batch_size):\n",
    "        '''\n",
    "        returns a single augmented image (x) with corresponding labels (y) in one-hot structure\n",
    "        '''\n",
    "        \n",
    "        x_data = np.zeros((batch_size, *self.patch_extractor.patch_size, 3))\n",
    "        y_data = np.zeros((batch_size, *self.target_size, 2)) # one-hot encoding with 2 classes\n",
    "        \n",
    "        for i in range(0, batch_size):\n",
    "        \n",
    "            random_index = np.random.choice(len(self.imgs)) # pick random image\n",
    "            img, lbl = self.imgs[random_index], self.lbls[random_index] # get image and segmentation map\n",
    "            patch_img, patch_lbl = self.patch_extractor.get_patch(img, lbl) # when image size is equal to patch size, this line is useless...\n",
    "        \n",
    "            # crop labels based on target_size\n",
    "            h, w, _ = patch_lbl.shape\n",
    "            ph = (self.patch_extractor.patch_size[0] - self.target_size[0]) // 2\n",
    "            pw = (self.patch_extractor.patch_size[1] - self.target_size[1]) // 2\n",
    "            x_data[i, :, :, :] = patch_img\n",
    "            y_data[i, :, :, 0] = 1 - patch_lbl[ph:ph+self.target_size[0], pw:pw+self.target_size[1]].squeeze()\n",
    "            y_data[i, :, :, 1] = patch_lbl[ph:ph+self.target_size[0], pw:pw+self.target_size[1]].squeeze()\n",
    "        \n",
    "        return (x_data.astype(np.float32), y_data.astype(np.float32))\n",
    "    \n",
    "    def get_image_generator(self, batch_size):\n",
    "        '''returns a generator that will yield image-batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_image_batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output of U-Net may not have the same size as its input, let's define a function ``pad_prediction()`` that visualizes labels adding padding, to match the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_prediction(prediction, input_size, pad_with=-1.0):\n",
    "    \"\"\"Only for visualization purpose, it introduces artificial -1.\"\"\"\n",
    "    pad_pred = pad_with * np.ones(input_size).astype(float)\n",
    "    pred_size = prediction.shape\n",
    "    D = ((input_size[0]-pred_size[0])//2, (input_size[1]-pred_size[1])//2)\n",
    "    pad_pred[D[0]:D[0]+pred_size[0], D[1]:D[1]+pred_size[1]] = prediction\n",
    "    return pad_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, manual annotations in the GlaS challenge come with a different label per gland instance. This is because the main task of the original GlaS challenge was instance segmentation, meaning segmentation of glands as well as detection of each gland as a separate object. We are not going to focus on the instance segmentation part for the time being, therefore we can just treat all manually annotated objects as belonging to the same class. Therefore, we have modified the labels in the dataset by just using label=1 for glands and label=0 for background. When we saved this map to disk, background got value 0 and foreground got value 255 (as you can see in the previous cells showing manual annotations of trainign and validation images). In order to be able ot use these annotations, we have to convert them to [0, 1] values. For this purpose, we define the following function, wich we will apply to the validation and to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_dataset(dataset): \n",
    "    return DataSet(dataset.imgs, [(x>0).astype(int) for x in dataset.lbls])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also we define a function that allows to apply the model to a given dataset, which will be used a lot to test your models and visualize results. Note that this function includes a parameter to make a submission file, which will be called ``results.zip``, to be submitted to grand-challenge. By default, this parameter is set to ``False``. Enable it later to make subsmission files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(model, dataset, experiment_name='basic_unet', make_submission_file=False):\n",
    "    \"\"\"Apply a given model to the test set, optionally makes a submission file in ZIP format.\"\"\"\n",
    "    \n",
    "    output_dir = os.path.join(data_dir, experiment_name)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for i in range(len(dataset.imgs)):\n",
    "        fig = plt.figure(figsize=(10,20))\n",
    "        input_img = np.expand_dims(dataset.imgs[i], axis=0)/255.\n",
    "        output = model.predict(input_img, batch_size=1)[0, :, :, :]\n",
    "        plt.subplot(1, 2, 1); plt.imshow(dataset.imgs[i])\n",
    "        plt.subplot(1, 2, 2); plt.imshow(np.argmax(output, axis=2))\n",
    "        if make_submission_file:\n",
    "            prediction = Image.fromarray(np.argmax(output, axis=2).astype(np.uint8))\n",
    "            prediction.save(os.path.join(output_dir, '{}.png'.format(i)))\n",
    "        plt.show()\n",
    "        \n",
    "    if make_submission_file:\n",
    "        shutil.make_archive('results', 'zip', output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define a Logger class that will store useful information during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(masks, lost_border):\n",
    "    ph = lost_border[0] // 2\n",
    "    pw = lost_border[1] // 2\n",
    "    h, w = masks[0].shape    \n",
    "    return np.array(masks)[:, ph:h-ph, pw:w-pw]   \n",
    "\n",
    "def calculate_dice(x, y):\n",
    "    '''returns the dice similarity score, between two boolean arrays'''\n",
    "    return 2 * np.count_nonzero(x & y) / (np.count_nonzero(x) + np.count_nonzero(y))\n",
    "    \n",
    "class Logger(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, validation_data, lost_border, data_dir, model_name):\n",
    "        self.val_imgs = np.array(validation_data.imgs) / 255.\n",
    "        self.val_lbls = crop(validation_data.lbls, lost_border)\n",
    "        self.model_filename = os.path.join(data_dir, model_name + '.h5')\n",
    "        \n",
    "        self.losses = []\n",
    "        self.dices = []\n",
    "        self.best_dice = 0\n",
    "        self.best_model = None\n",
    "        \n",
    "        self.predictions = None\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.losses.append(logs.get('acc'))\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        dice = self.validate()\n",
    "        self.dices.append([len(self.losses), dice])\n",
    "        if dice > self.best_dice:\n",
    "            self.best_dice = dice\n",
    "            self.model.save(self.model_filename) # save best model to disk\n",
    "            print('best model saved as {}'.format(self.model_filename))\n",
    "        self.plot()   \n",
    "    \n",
    "    def validate(self):\n",
    "        self.predictions = self.model.predict(self.val_imgs, batch_size=1)\n",
    "        predicted_lbls = np.argmax(self.model.predict(self.val_imgs, batch_size=1), axis=3)\n",
    "        x = self.val_lbls>0\n",
    "        y = predicted_lbls>0\n",
    "        return calculate_dice(x, y)\n",
    "    \n",
    "    def plot(self):\n",
    "        clear_output()\n",
    "        N = len(self.losses)\n",
    "        plt.figure(figsize=(50, 10))\n",
    "        plt.subplot(1, 5, 1)\n",
    "        plt.plot(range(0, N), self.losses); plt.title('losses')\n",
    "        plt.subplot(1, 5, 2)\n",
    "        plt.plot(*np.array(self.dices).T); plt.title('dice')\n",
    "        plt.subplot(1, 5, 3)\n",
    "        plt.imshow(self.val_imgs[1]); plt.title('RGB image')\n",
    "        plt.subplot(1, 5, 4)\n",
    "        plt.imshow(np.argmax(self.predictions[1], axis=2)); plt.title('prediction')\n",
    "        plt.subplot(1, 5, 5)\n",
    "        plt.imshow(self.val_lbls[1]); plt.title('ground truth')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define a function ``train_model`` that trains our model using training parameters and training/validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train a model\n",
    "def train_model(model, training_params):\n",
    "    \n",
    "    patch_size = training_params['patch_size']\n",
    "    target_size = training_params['target_size']\n",
    "    batch_size = training_params['batch_size']\n",
    "    loss = training_params['loss']\n",
    "    metrics = training_params['metrics']\n",
    "    logger = training_params['logger']\n",
    "    epochs = training_params['epochs']\n",
    "    steps_per_epoch = training_params['steps_per_epoch']\n",
    "    optimizer = training_params['optimizer']\n",
    "    training_dataset = training_params['training_dataset']\n",
    "    validation_dataset = training_params['validation_dataset']\n",
    "        \n",
    "    # batch generator \n",
    "    patch_generator = PatchExtractor(patch_size)\n",
    "    batch_generator = BatchCreator(patch_generator, training_dataset, target_size=target_size)\n",
    "    image_generator = batch_generator.get_image_generator(batch_size)\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    # train the model\n",
    "    model.fit_generator(generator=image_generator, \n",
    "                        steps_per_epoch=steps_per_epoch, \n",
    "                        epochs=epochs,\n",
    "                        callbacks=[logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define datasets\n",
    "Now we can use the tools that we have initialized to build the datasets that we will use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define directory for training and test data, as they were created after downloading data from SURFDrive\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "# load images + manual annotations from the training set, images from the test set, and store them in lists.\n",
    "# Note that 85 and 80 are hard-coded numbers here. You may have to change this if you change dataset in the future.\n",
    "train_imgs = [load_img(os.path.join(train_dir, '{}.png'.format(f+1))) for f in range(85)]\n",
    "train_lbls = [load_img(os.path.join(train_dir, '{}_anno.png'.format(f+1))) for f in range(85)]\n",
    "test_imgs = [load_img(os.path.join(test_dir, '{}.png'.format(f))) for f in range(80)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images in the training set are currently in the same order as read from disk. To avoid any bias possibly introduced by this order, it is always good to shuffle the training dataset. Note that images and annotations must be shuffled in the same way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle order of training images and manual annotations\n",
    "indexes = list(range(85))\n",
    "random.shuffle(indexes)\n",
    "train_imgs = list(np.asarray(train_imgs)[indexes])\n",
    "train_lbls = list(np.asarray(train_lbls)[indexes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and validation set\n",
    "Now we can define a training and a validation set by using the Dataset class that we have defined. In order to define a validation set, you have to specify a coefficient (from 0 to 1) to indicate the percentage of training images that you want to use for validation (we have seen that typical splits are 70/30, 80/20 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_percent = None # coefficient to define validation dataset (value between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_validation_imgs = int(validation_percent * len(train_imgs))\n",
    "\n",
    "# use the first images as validation\n",
    "validation_dataset = DataSet(train_imgs[:n_validation_imgs], train_lbls[:n_validation_imgs])\n",
    "\n",
    "# the rest as training\n",
    "training_dataset = DataSet(train_imgs[n_validation_imgs:], train_lbls[n_validation_imgs:])\n",
    "\n",
    "# test dataset\n",
    "test_dataset = DataSet(test_imgs)\n",
    "\n",
    "n_tra_imgs = training_dataset.get_lenght()\n",
    "n_val_imgs = validation_dataset.get_lenght()\n",
    "n_tes_imgs = test_dataset.get_lenght()\n",
    "\n",
    "print('{} training images'.format(n_tra_imgs))\n",
    "print('{} validation images'.format(n_val_imgs))\n",
    "print('{} test images'.format(n_tes_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize datasets\n",
    "Let's visualize images in the training, the validation and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training set\n",
    "for i in range(n_tra_imgs):\n",
    "    training_dataset.show_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# validation set\n",
    "for i in range(n_val_imgs):\n",
    "    validation_dataset.show_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "for i in range(n_tes_imgs):\n",
    "    test_dataset.show_image(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that manual annotations loaded from disk are in the range [0, 255], because they were stored as grayscale images, where white has an intensity of 255. For training purposes, we need to have labels in the range [0, 1]. Therefore, we have to apply the function ``binarize_dataset`` that we have defined before. After applying it, the range of values of manual annotations should be [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set labels to 0 or 1 in training and validation datasets\n",
    "training_dataset = binarize_dataset(training_dataset)\n",
    "validation_dataset = binarize_dataset(validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the training and validation sets again to check that things are correct now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize training set\n",
    "for i in range(n_tra_imgs):\n",
    "    training_dataset.show_image(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize validation set\n",
    "for i in range(n_val_imgs):\n",
    "    validation_dataset.show_image(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test batch generator\n",
    "Before we delve into training U-Net, it is good to test whether the patch generator and the batch generator are working as expected (for example, check if the one-hot encoding is working as expected). Let's test them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define parameters for patch generator and batch creator\n",
    "patch_size = (256, 256) # input size\n",
    "target_size = (256, 256) # output size, might be the same as input size, but might be smaller, if valid convolutions are used\n",
    "batch_size = 16 # number of patches in a mini-batch\n",
    "\n",
    "# intialize patch generator and batch creator\n",
    "patch_generator = PatchExtractor(patch_size)\n",
    "batch_generator = BatchCreator(patch_generator, training_dataset, target_size=target_size)\n",
    "\n",
    "# get one minibatch\n",
    "x_data, y_data = batch_generator.create_image_batch(batch_size)\n",
    "\n",
    "print(x_data.shape)\n",
    "print(y_data.shape)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(x_data[i]); plt.title('RGB image')\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(pad_prediction(y_data[i, :, :, 0].squeeze(), patch_size)); plt.title('Label map class 0')\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(pad_prediction(y_data[i, :, :, 1].squeeze(), patch_size)); plt.title('Label map class 1')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all we need to start building our prediction model with training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a baseline U-Net model (U-Net 1)\n",
    "Here we explicitly define a baseline U-Net model. It has a depth of 4 (3 pooling layers), it uses 'same' convolutions, it has 16 filters in the first layer, it accepts inputs of 256x256 RGB images, so it is a very simplified version of the original U-Net. For convenience, we call it ``unet_1``, to differentiat it from other U-Net models that will be built in the rest of the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def build_unet_1(printmodel=False):\n",
    "    \n",
    "    inputs = Input(shape=(256, 256, 3))\n",
    "\n",
    "    # First conv pool\n",
    "    c1 = Conv2D(16, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = Conv2D(16, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = MaxPooling2D()(c1)\n",
    "\n",
    "    # Second conv pool\n",
    "    c2 = Conv2D(32, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = Conv2D(32, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = MaxPooling2D()(c2)\n",
    "\n",
    "    # Third conv pool\n",
    "    c3 = Conv2D(64, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = Conv2D(64, 3, activation='relu', padding='same')(c3)\n",
    "    p3 = MaxPooling2D()(c3)\n",
    "\n",
    "    # Fourth conv pool\n",
    "    c4 = Conv2D(128, 3, activation='relu', padding='same')(p3)\n",
    "    c4 = Conv2D(128, 3, activation='relu', padding='same')(c4)\n",
    "\n",
    "    # First up-conv\n",
    "    u2 = UpSampling2D()(c4)\n",
    "    m2 = concatenate([c3, u2])\n",
    "    cm2 = Conv2D(64, 3, activation='relu', padding='same')(m2)\n",
    "    cm2 = Conv2D(64, 3, activation='relu', padding='same')(cm2)\n",
    "\n",
    "    # Second up-conv\n",
    "    u3 = UpSampling2D()(cm2)\n",
    "    m3 = concatenate([c2, u3])\n",
    "    cm3 = Conv2D(32, 3, activation='relu', padding='same')(m3)\n",
    "    cm3 = Conv2D(32, 3, activation='relu', padding='same')(cm3)\n",
    "\n",
    "    # Third up-conv\n",
    "    u4 = UpSampling2D()(cm3)\n",
    "    m4 = concatenate([c1, u4])\n",
    "    cm4 = Conv2D(16, 3, activation='relu', padding='same')(m4)\n",
    "    cm4 = Conv2D(16, 3, activation='relu', padding='same')(cm4)\n",
    "\n",
    "    # Output\n",
    "    predictions = Conv2D(2, 1, activation='softmax')(cm4)\n",
    "\n",
    "    model = Model(inputs, predictions)\n",
    "    \n",
    "    if printmodel:\n",
    "        print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an instance of the ``unet_1`` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_1 = build_unet_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the performance of this U-Net with randomly initialized parameters? Let's find it out (at least, visually) by running the model on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the model to the validation set\n",
    "apply_model(unet_1, validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the output of randomly initialized parameters is of course very bad.\n",
    "In the next cell, we initialize all training parameters, define an image generator that will be able to return mini-batches during the training procedure, define a loss function, the mini-batch size, and other parameters needed to train U-Net. Then, we train the model using the ``train_model`` function defined above. Replace ``None`` with some values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "\n",
    "model_name = 'unet_1'\n",
    "\n",
    "training_params = {}\n",
    "training_params['learning_rate'] = None\n",
    "training_params['patch_size'] = (256, 256) # input size\n",
    "training_params['target_size'] = (256, 256) # output size, might be the same as input size, but might be smaller, if valid convolutions are used\n",
    "training_params['batch_size'] = None # number of patches in a mini-batch\n",
    "training_params['steps_per_epoch'] = None # number of iterations per epoch\n",
    "training_params['epochs'] = None # number of epochs\n",
    "\n",
    "training_params['optimizer'] = SGD(lr=training_params['learning_rate'], momentum=0.9, nesterov=True)\n",
    "\n",
    "training_params['loss'] = ['categorical_crossentropy']\n",
    "training_params['metrics'] = ['accuracy']\n",
    "training_params['training_dataset'] = training_dataset\n",
    "training_params['validation_dataset'] = validation_dataset\n",
    "\n",
    "# initialize a logger, to keep track of information during training\n",
    "lost_border = ((training_params['patch_size'][0]-training_params['target_size'][0])//2, (training_params['patch_size'][1]-training_params['target_size'][1])//2)\n",
    "training_params['logger'] = Logger(validation_dataset, lost_border, data_dir, model_name)\n",
    "\n",
    "# train model\n",
    "train_model(unet_1, training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can load the best saved model and apply it to the validation set and check the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model (which gave best Dice score on validation set during training)\n",
    "unet_1 = load_model(os.path.join(data_dir, model_name + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply best model to the validation set first\n",
    "apply_model(unet_1, validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the function ``apply_model()`` also to process the test set and (optionally) make a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply best model to test set and make a submission file\n",
    "apply_model(unet_1, test_dataset, 'unet_1', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now download this zipfile with this link: [results.zip](results.zip).  \n",
    "Next, upload your result to the challenge website (https://ismi-glas.grand-challenge.org/evaluation/results/).\n",
    "\n",
    "You have realized that this baseline model does not perform well at all!\n",
    "You can of course play around with ome hyperparameters (learning rate, mini-batch size, etc.) and try to improve performance. We tried a bit and with this architecture we could not get very good performance. The good news is that now you have an example and a full pipeline that you can use to build your own U-Net model and improve the performance! In the next cells, we will provide some guidance how to make U-Net work better, starting with adding batch normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement U-Net blocks and add Batch Normalization (U-Net 2)\n",
    "You have probably realized that a U-Net model can be built by writing a simple program, because U-Net blocks have all the same structure, which can be coded as a function. Here we implement U-Net blocks, which will make the design of more U-Net architectures in this assignment much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that builds a U-Net block, containing conv->(batchnorm->)conv->(batchnorm),\n",
    "# where batchnorm is optional and can be selected via input parameter.\n",
    "# The function returns the output of a convolutional (or batchnorm) layer \"cl\"\n",
    "def unet_block(inputs, n_filters, batchnorm=False):\n",
    "    \n",
    "    # >> YOUR CODE HERE <<\n",
    "    \n",
    "    return cl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use ``unet_block()`` to build a U-Net model with a script: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_2(initial_filters=16, n_classes=2, batchnorm=False, printmodel=False):\n",
    "\n",
    "    # build U-Net again using unet_block function\n",
    "    inputs = Input(shape=(256, 256, 3))\n",
    "\n",
    "    # CONTRACTION PART\n",
    "\n",
    "    # First conv pool\n",
    "    c1 = unet_block(inputs, initial_filters, batchnorm)\n",
    "    p1 = MaxPooling2D()(c1)\n",
    "\n",
    "    # Second conv pool\n",
    "    c2 = unet_block(p1, 2*initial_filters, batchnorm)\n",
    "    p2 = MaxPooling2D()(c2)\n",
    "\n",
    "    # Third conv pool\n",
    "    c3 = unet_block(p2, 4*initial_filters, batchnorm)\n",
    "    p3 = MaxPooling2D()(c3)\n",
    "\n",
    "    # Fourth conv\n",
    "    c4 = unet_block(p3, 8*initial_filters, batchnorm)\n",
    "\n",
    "    # EXPANSION PART\n",
    "\n",
    "    # First up-conv\n",
    "    u2 = UpSampling2D()(c4)\n",
    "    m2 = concatenate([c3, u2])\n",
    "    cm2 = unet_block(m2, 4*initial_filters, batchnorm)\n",
    "\n",
    "    # Second up-conv\n",
    "    u3 = UpSampling2D()(cm2)\n",
    "    m3 = concatenate([c2, u3])\n",
    "    cm3 = unet_block(m3, 2*initial_filters, batchnorm)\n",
    "\n",
    "    # Third up-conv\n",
    "    u4 = UpSampling2D()(cm3)\n",
    "    m4 = concatenate([c1, u4])\n",
    "    cm4 = unet_block(m4, initial_filters, batchnorm)\n",
    "\n",
    "    # Output\n",
    "    predictions = Conv2D(n_classes, 1, activation='softmax')(cm4)\n",
    "\n",
    "    model = Model(inputs, predictions)\n",
    "    \n",
    "    if printmodel:\n",
    "        print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a second model using the function we just made, when batch normalization is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_2 = build_unet_2(batchnorm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "\n",
    "model_name = 'unet_2'\n",
    "\n",
    "training_params = {}\n",
    "training_params['learning_rate'] = 0.0001\n",
    "training_params['patch_size'] = (256, 256) # input size\n",
    "training_params['target_size'] = (256, 256) # output size, might be the same as input size, but might be smaller, if valid convolutions are used\n",
    "training_params['batch_size'] = 16 # number of patches in a mini-batch\n",
    "training_params['steps_per_epoch'] = 10\n",
    "training_params['epochs'] = 20\n",
    "training_params['optimizer'] = SGD(lr=training_params['learning_rate'], momentum=0.9, nesterov=True)\n",
    "training_params['loss'] = ['categorical_crossentropy']\n",
    "training_params['metrics'] = ['accuracy']\n",
    "training_params['training_dataset'] = training_dataset\n",
    "training_params['validation_dataset'] = validation_dataset\n",
    "\n",
    "# initialize a logger, to keep track of information during training\n",
    "lost_border = ((training_params['patch_size'][0]-training_params['target_size'][0])//2, (training_params['patch_size'][1]-training_params['target_size'][1])//2)\n",
    "training_params['logger'] = Logger(validation_dataset, lost_border, data_dir, model_name)\n",
    "\n",
    "# train model\n",
    "train_model(unet_2, training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model (which gave best Dice score on validation set during training)\n",
    "unet_2 = load_model(os.path.join(data_dir, model_name + '.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply model to test set and save results as results.zip\n",
    "apply_model(unet_2, test_dataset, experiment_name='unet_2', make_submission_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to change the number of filters per layer by passing a different input parameter ``initial_filters`` (default = 16) to ``build_unet_2()``, and experiment the effect of using a wider U-Net model. Repeat the experiments done before (training and inference on the test set + submission to grand-challenge) by adding more cells below. **Do not modify previous cells, because this would make reading your assignment very difficult!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_filters = None\n",
    "unet_2_wider = build_unet_2(initial_filters=initial_filters, batchnorm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Regularization (U-Net 3)\n",
    "You have probably seen that batch normalization helps improve the performance. One reason could be because beatchnorm has some regularization effect. Maybe you have also noticed that we haven't used explicit regularization terms so far, as for example dropout layers, or L1/L2 losses. Now we want to do that and investigate how/whether performance improve.\n",
    "\n",
    "For this purpose, make new functions ``unet_block_dropout`` and ``build_unt_3`` to add Dropout and L1/L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_block_dropout(inputs, n_filters, batchnorm=False, dropout_rate=0.5):\n",
    "    \n",
    "    cl = None\n",
    "    \n",
    "    # >>> YOUR CODE HERE <<<\n",
    "    \n",
    "    return cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_3(initial_filters=16, n_classes=2, batchnorm=False, printmodel=False):\n",
    "    \n",
    "    # >>> YOUR CODE HERE <<<\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add weight map\n",
    "As seen in the lecture this week, U-Net uses weights in its loss function to take into account for (1) different frequency of labels and (2) distance of pixels from objects to segment. The first component is useful to compensate for skewed distributions of pixels in training samples (remember that in U-Net every pixel count as one sample in the loss function). The second component helps to enforce separation of objects in the segmentation map. In this assignment, we only focus on the first term, and we further simplify it by only computing the proportion of pixels in the entire training set, instead of for each training sample. For this purpose, the option ``class_weight`` of ``fit_generator()`` can be used (check out the Keras documentation here: https://keras.io/models/model/#fit_generator). Note that very small or very large weights can hamper the training procedure. This about the best way to define ``w_background`` and ``w_foreground``. For this purpose, you will have to modify the function ``train_model``, where ``fit_generator`` is used. When you modify the function, make sure that it is still compatible with the resto fo your code (all previous cells that do not use ``class_weigths``)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(training_dataset):\n",
    "    \n",
    "    w_background = None\n",
    "    w_foreground = None\n",
    "    \n",
    "    # >>> YOUR CODE HERE <<<\n",
    "    \n",
    "    return w_background, w_foreground"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we assume you will be using ``unet_3`` as reference model, but feel free to use any other model you have developed so far, the main idea of this experiment is to check whether using class weights improve the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "train_model(unet_3, training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use valid convolutions\n",
    "So far, we have been using ``same`` convolutions in our U-Net model, which makes easy to control the concatenation of feature maps in the contraction and in the extraction paths. However, the original model didn't use same convolutions but instead used valid convolutions. This means that the netowkr architecture and the input size have to be considered at the same time, and set the proper parameters. Additionally, feature maps have to be cropped before concatenation.\n",
    "\n",
    "The design of such a network requires considering several components and might be tricky. Therefore, we add this task as final and optional one, which will give you 10 extra points.\n",
    "\n",
    "Concisdering that the (maximum) input size is 256x256, you should build a U-Net with:\n",
    "* a depth of 4 (3 pooling layers)\n",
    "* valid convolutions\n",
    "* central cropping\n",
    "* input size <= 256 (try to find the best combination)\n",
    "\n",
    "In this final part of the assignment, you will have to use some of the functions that we have defined but not used yet, to pad and crop, and you should also modify the batch generator to return patches of different size (if different from 256); in that case, you can simply do central cropping in the batch generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_block_valid(inputs, n_filters, batchnorm=False, dropout_rate=0.5):\n",
    "    \n",
    "    cl = None\n",
    "    \n",
    "    # >>> YOUR CODE HERE <<<\n",
    "    \n",
    "    return cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet_valid():\n",
    "    \n",
    "    # >>> YOUR CODE HERE <<<\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
