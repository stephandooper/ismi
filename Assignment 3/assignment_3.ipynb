{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching Assistants:\n",
    "- Ecem Lago: ecem.lago@radboudumc.nl\n",
    "- Thomas de Bel: Thomas.deBel@radboudumc.nl\n",
    "\n",
    "Please submit your notebook via grand-challenge.org: https://ismi-nodules.grand-challenge.org/\n",
    "\n",
    "This notebook requires tensorflow and keras installed, please make sure you installed the required packages: https://ismi-nodules.grand-challenge.org/Resources/\n",
    "\n",
    "Submit a notebook **WITH ALL CELLS EXECUTED!!!**\n",
    "\n",
    "* Groups: You should work in pairs or alone. Working in groups of 2-3 is preferable.\n",
    "* Deadline for this assignment: \n",
    " * Monday, February 18th until 23:59h.\n",
    " * 5 points (maximum grade = 100 points) penalization per day after deadline.\n",
    "* Submit your **fully executed** notebook to the grand-challenge.org platform.\n",
    "* The file name of the notebook you submit must be ```NameSurname1_NameSurname2_NameSurname3.ipynb```\n",
    "* The grades will be available before February 25th (tentative)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/lungrads.png\" alt=\"LungRADS guidelines\" align=\"right\" width=\"500\">\n",
    "\n",
    "A pulmonary nodule is a small round or oval-shaped growth in the lung. It may also be called a “spot on the lung” or a “coin lesion.” Pulmonary *nodules* are smaller than three centimeters (around 1.2 inches) in diameter. If the growth is larger than that, it is called a pulmonary *mass* and is more likely to represent a cancer than a nodule [http://my.clevelandclinic.org/health/articles/pulmonary-nodules].\n",
    "\n",
    "Nodules can be detected in chest CT images as objects with some kind of rounded shape (even though it is not always the case), which have an intensity that is higher (brighter) than the parenchyma tissue in the lungs.\n",
    "\n",
    "If a nodule is detected, guidelines have to be followed to decide what is the best management for the patient.\n",
    "For this purpose, the LungRADS guidelines have been released [https://www.acr.org/Clinical-Resources/Reporting-and-Data-Systems/Lung-Rads], which describe the type of follow-up analysis based on the type and size of detected nodules.\n",
    "The main categories of nodules considered in LungRADS are 5:\n",
    "* solid nodule\n",
    "* ground-glass nodules (also called GGN, non-solid nodules)\n",
    "* semi-solid nodules (also called part-solid nodules)\n",
    "* calcified nodules\n",
    "* spiculated nodules\n",
    "\n",
    "<img src=\"./figures/nodules.png\" alt=\"Nodules\" align=\"right\" width=\"500\">\n",
    "\n",
    "**Solid** nodules are characterized by an homogeneous texture, a well-defined shape and an intensity above -450\n",
    "Housfield Units (HU) on CT. **Spiculated** nodules appear as solid lesions with characteristics spikes at the border, often considered as an indicator of malignancy. **Non-Solid** nodules (also called ground-glass opacities) have an intensity on CT lower than solid nodules (above -750 HU). **Part-Solid** nodules (also called semi-solid nodules) contain both a non-solid and a solid part, the latter normally referred to as the solid core. Compared with solid nodules, non-solid and part-solid nodules have a higher frequency of being malignant lesions. Finally, **calcified** nodules are characterized by a high intensity and a well-defined rounded shape on CT. If a nodule is completely calcified, it is a benign lesion.\n",
    "\n",
    "The figure on the right shows the table used in LungRADS, which you can also find in ```./literature/AssessmentCategories.pdf```.\n",
    "As you can see, the five categories are mentioned in the table, as well as nodule size.\n",
    "While nodule size is something that can be easily measured using a segmentation software, the discrimination of nodule types is not trivial. The figure on the right shows examples of pulmonary nodules at different scales. For each nodule, a 2D view in the axial, coronal and sagittal view is shown. \n",
    "\n",
    "In this assignment, we are going to develop a system based on machine learning to automatically classify pulmonary nodules detected in chest CT scans. For this purpose, we will use data from the publicly available dataset LIDC-IDRI (https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI). In LIDC-IDRI, nodules have been annotated and labeled by four radiologists. Based on their annotations, we extracted a subset of nodules that will be used in this assignment for training and for test purposes.\n",
    "\n",
    "The idea of this assignment is to develop a **multi-class classification system** using machine learning, in particular using **neural networks**. The goal is to achieve the best classification accuracy on the test set, which contains 50 nodules for each class. For each nodule in both the training and test set, we provide both raw data (cubes of 40x40x40 mm containing nodules) and a representation of nodules, meaning a feature vector of 256 values (more details are provided later).\n",
    "The purpose of this assignment is two-fold:\n",
    "\n",
    "1. Use the features provided to develop a system based on neural networks to classify pulmonary nodule type\n",
    "2. Modify the architecture and the hyper-parameters of the neural networks and investigate how performance change\n",
    "3. Design and extract new features from raw data, and use them in your classification framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use data from the publicly available LIDC-IDRI dataset (https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI).\n",
    "From this dataset, we selected nodules to use in this assignment. In particular, we made a training set and a test set.\n",
    "In both sets, five nodule types are included, namely:\n",
    "* Solid\n",
    "* Non-solid (ground-glass)\n",
    "* Part-solid (semi-solid)\n",
    "* Calcified\n",
    "* Solid spiculated\n",
    "\n",
    "**Training set**. The training set **contains a distribution of nodules types that resembles what is typically found in radiology: most of the nodules are solid**. As a consequence, you will notice that the distributions of classes are skewed.\n",
    "\n",
    "**Validation set**.\n",
    "We don't provide a validation set. As part of the assignment, you will be asked to build a validation set yourself.\n",
    "\n",
    "**Test set**.\n",
    "The test set contains a **balanced** distribution of nodules, meaning that we randomly selected the same amount of nodules per class.\n",
    "\n",
    "### Data details\n",
    "We provide training and test data in two formats: (a) a **dataset** format and (b) a **raw data** format.\n",
    "\n",
    "#### Dataset format\n",
    "The first format is a typical **dataset** format, a matrix, where each row is a sample and each column is a feature. In this\n",
    "dataset, we provide 256 features per nodule. At this stage you don't need to care too much about the source of these features (we will tell you more about that later), just consider that these features are somehow descriptive of pulmonary nodules.\n",
    "The files in dataset format are ```training_set.npz``` and ```test_set.npz```.\n",
    "In the first part of this assignment, you will be asked to train and validate neural networks using the features provided.\n",
    "The training dataset contains data in the field ```['x']``` and labels in the field ```['y']```, while the test set only contains data in the field ```['x']```. The test set also contains a fiels ```['nodule_ids']```, which has unique nodule identifiers that will be used to build the file to submit to *grand-challenge.org*.\n",
    "The nodule ID has the following format:\n",
    "\n",
    "* ```seriesuid_worldx_worldy_worldz_diameter```\n",
    "\n",
    "where ```seriesuid``` is the code that identifies the scan, ```worldx```, ```y``` and ```z``` indicate the position of the nodule in the scan in world coordinates, and ```diameter``` indicates the diameter of the nodule in *mm*. The position is not really important in this assignment, since we extracted the nodules from the scans for you, but in case you want to trace back these nodules, you know where you can find them in the LIDC-IDRI scans. The diameter may be useful in the last part of this assignment.\n",
    "\n",
    "#### Raw data format\n",
    "We also provide raw nodule data, because in the third part of this assignment, you will be asked to train and validate neural networks using raw nodule data. In this part of the assignment, raw data will be avaialble, meaning that you will have the chance of processing data as you like, directly feeding data to the neural network, or extracting additional features, etc.\n",
    "Since pulmonary nodules are extracted from CT scans, raw data is 3D data.\n",
    "Therefore, what we provide is a *cube* for each nodules with size 64x64x64 px, which corresponds to 40x40x40*mm* (we resampled the scans before nodule extraction).\n",
    "\n",
    "In the training set, raw nodule data is organized in folders grouped per nodule type. We think that this may be convenient for you. Cubes are stored as ```npz``` files, and HU intensities are stored in the field ```['data']```, while the class label is stored in the field ```['label']``` for each nodule. Each file is named using the ```nodule_id```, where we also append the actual nodule label at the end of the file name.\n",
    "\n",
    "In the test set, you fill find a similar structure as for the training set, but labels are not provided and nodules are not organized in folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tasks\n",
    "The tasks for this assignment are the following.\n",
    "\n",
    "\n",
    "### 1. Train a basic neural network with given nodule-related features (60 points)\n",
    "We have computed features of nodules that you can use for this first part of the assignment. The feature vector contains 256 features per sample, you can find them in ```LIDC-IDRI/training/training_set.npz``` and ```LIDC-IDRI/test/test_set.npz```.\n",
    "* Report results using the shallow neural networks architecture proposed in this first part of the assignment.\n",
    "\n",
    "### 2. Improve the neural network by adding more layers (20 points)\n",
    "For the second part of the assignment you need to improve the performance of your network. You can do this, for example, by adding extra layers, or by fine-tuning the hyper-parameters.\n",
    "* Report results using neural networks, experiment with several architectures, hyper-parameters, try with different learning rates, etc.\n",
    "\n",
    "### 3. Train a neural network with raw nodule data (20 points)\n",
    "For the thrid part of the assignment you will have to deal with raw data, you can find them in the directory ```nodules``` for training and test respectively.\n",
    "* Report results using neural networks, experiment with several architectures, hyper-parameters\n",
    "* Design and extract your own features from the raw data provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Train a neural network with given nodule-related features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "import sklearn.neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import keras    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset on your computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm_notebook\n",
    "import zipfile\n",
    "\n",
    "link = 'https://surfdrive.surf.nl/files/index.php/s/IglByZUcav5BTtv/download'\n",
    "file_name = \"LIDC-IDRI.zip\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in tqdm_notebook(response.iter_content(chunk_size=4096), desc='Downloading data'):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "with zipfile.ZipFile(file_name,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "os.remove('./LIDC-IDRI.zip')\n",
    "data_dir = './LIDC-IDRI'\n",
    "network_dir = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels for the five classes in this classification problem.\n",
    "# These are the same names used in folders for raw training data\n",
    "noduleTypes = [\"Solid\", \"Calcified\", \"PartSolid\", \"GroundGlassOpacity\", \"SolidSpiculated\"]\n",
    "n_classes = len(noduleTypes)# len returns the number of element in a list\n",
    "print('Classification problem with {} classes'.format(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenience function\n",
    "def get_file_list(path,ext='',queue=''):\n",
    "    if ext != '': return [os.path.join(path,f) for f in os.listdir(path) if f.endswith(''+queue+'.'+ext+'')],  [f for f in os.listdir(path) if f.endswith(''+queue+'.'+ext+'')]    \n",
    "    else: return [os.path.join(path,f) for f in os.listdir(path)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get to know your data!\n",
    "The data you have now is stored in cubes per nodule, but originally belongs <img src=\"figures/orthogonalviews.jpg\" align='right'> to a Chest CT scan. We have three orthogonal orientations: axial, coronal and sagittal orientations. Each cube is $64\\times64\\times64$ px (40x40x40 *mm*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that, given a cube containing nodule raw 3D data, returns three orthogonal 2D patches, corresponding to the *axial*, *coronal* and *sagittal* view. Use the figure on the right in this notebook as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convenience function that returns the axial, coronal and sagitaal view given a cube (in numpy format) containing a nodule\n",
    "def get_ortogonal_patches(x):\n",
    "    \n",
    "    dims = x.shape # Get shape of X\n",
    "    \n",
    "    ### modify the next three lines of code to extract three views of a nodule ###\n",
    "    axial = x[None, None, None].squeeze()\n",
    "    coronal = x[None, None, None].squeeze()\n",
    "    sagittal= x[None, None, None].squeeze()\n",
    "    \n",
    "    return axial, coronal, sagittal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now visualize some nodules in the training set in axial, coronal and sagittal orientations using the function that you have implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_dir = os.path.join(data_dir, \"training\", \"nodules\")\n",
    "for noduleType in (noduleTypes):\n",
    "    nodules_dir = os.path.join(src_dir, noduleType)\n",
    "    npzs = get_file_list(nodules_dir, 'npz')\n",
    "    \n",
    "    ## Own implementation\n",
    "    for idx1, f in enumerate(range(len(npzs[0]))):\n",
    "        if idx1 > 5:\n",
    "            break\n",
    "        file_path = npzs[0][f]\n",
    "        filename = npzs[1][f]\n",
    "        # axes are oriented as (z, x, y)\n",
    "        npz = np.load(file_path)\n",
    "        axial, coronal, sagittal = get_ortogonal_patches(npz['data'])\n",
    "        print(sagittal.shape,'axial shape')\n",
    "        plt.suptitle(noduleType)\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.imshow(axial   , cmap='gray', vmin=-600-800, vmax=-600+800); plt.title('axial')\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(coronal , cmap='gray', vmin=-600-800, vmax=-600+800); plt.title('coronal')\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.imshow(sagittal, cmap='gray', vmin=-600-800, vmax=-600+800); plt.title('sagittal')\n",
    "        plt.show()\n",
    "        \n",
    "shape_cube = npz['data'].shape\n",
    "print ('Size of our cubes',shape_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "The quality of patches looks better in the axial view than in the coronal and sagittal view. Why do you think we get this difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets\n",
    "Here we are going to load the pre-extracted feature vector of 256 features per sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data (given features)\n",
    "npz = np.load(os.path.join(data_dir, 'training', 'training_set.npz'))\n",
    "x = npz['x']\n",
    "y = npz['y']\n",
    "print (x.shape)\n",
    "print (y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data (given features)\n",
    "npz = np.load(os.path.join(data_dir, 'test', 'test_set.npz') )\n",
    "x_test = npz['x']\n",
    "nodule_ids_test = npz['nodule_ids']\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we learned last week, it is always good to pre-process our data to have zero mean and unit standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize training data\n",
    "x_mean = np.mean(x, axis=0)\n",
    "x_std  = np.std(x, axis=0)\n",
    "x_norm = (x - x_mean)/x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize test data\n",
    "x_mean = np.mean(x_test, axis=0)\n",
    "x_std  = np.std(x_test, axis=0)\n",
    "x_test = (x_test - x_mean)/x_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and validation sets\n",
    "\n",
    "The dataset we are given does not explicitly provide a validation set.\n",
    "Therefore, we will have to define one, and consequently update the training set (remove samples used for validation, why?).\n",
    "\n",
    "In the following cell, you will implement a function that derives a validation set and a new training set from a given (original) training set. You will use this function to define the subsets that you will be using during training.\n",
    "The function should derive the amount of samples it should take for the validation set from your dataset. Then, sample an equal amount of samples for each class from your dataset. As we don't want any unneccesarry copies of our data, we want to extract the data directly from the input using the indexes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split training set into training and validation subsets\n",
    "def split_training_validation_datasets(x, y, val_percentage=0.3, val_balanced=True):\n",
    "    \"\"\"\n",
    "    Derive a training and a validation datasets from a given dataset with\n",
    "    data (x) and labels (y). By default, the validation set is 30% of the\n",
    "    training set, and it has balanced samples across classes. When balancing,\n",
    "    it takes the 30% of the class with less samples as reference.\n",
    "    \"\"\"\n",
    "    \n",
    "    # define number of samples\n",
    "    n_samples = x.shape[0]\n",
    "    \n",
    "    # make array of indexes of all samples [0, ..., n_samples -1]\n",
    "    idxs = np.array(range(n_samples))\n",
    "    \n",
    "    # initialize (empty) lists of samples that will be part of training and validation sets \n",
    "    tra_idxs = []\n",
    "    val_idxs = []\n",
    "    \n",
    "    # append values to tra_idxs and val_idxs by adding the index of training and validation samples\n",
    "    # take into account the input parameters 'val_percentage' and 'val_balanced'\n",
    "\n",
    "    ### YOUR CODE HERE ###\n",
    "        \n",
    "    # print number of samples in training and validation sets\n",
    "    print('validation samples = {}'.format(len(val_idxs)))\n",
    "    print('training samples   = {}'.format(len(tra_idxs)))\n",
    "        \n",
    "    # define training/validation data and labels as subsets of x and y\n",
    "    x_train = x[tra_idxs]\n",
    "    y_train = y[tra_idxs]\n",
    "    x_validation = x[val_idxs]\n",
    "    y_validation = y[val_idxs]\n",
    "    \n",
    "    return x_train, y_train, x_validation, y_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "x_train, y_train, x_validation, y_validation = split_training_validation_datasets(x_norm, y, n_classes)\n",
    "print(y_train)\n",
    "print(y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffle your training set!** This is necessary when we want to train a neural network using stochastic (mini-batch) gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indx=list(range(x_train.shape[0]))\n",
    "print (y_train)\n",
    "np.random.shuffle(indx)\n",
    "x_train = x_train[indx]\n",
    "y_train = y_train[indx]\n",
    "print (y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question\n",
    "Why do you think it is important to shuffle the dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN classifier\n",
    "\n",
    "Now that we have defined a training and a validation set, we can define a baseline result by applying kNN classifier (which we have seen in previous assignments), and compute the accuracy on the validation set. If you have made a balanced validation set, accuracy is a good evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN\n",
    "classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=30)\n",
    "classifier.fit(x_train, y_train)\n",
    "y_validation_auto = classifier.predict(x_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the confusion matrix for the results with kNN classifier. In order to compute the **confusion matrix** and the **accuracy**, you can use functions from the [sklearn library](http://scikit-learn.org/stable/documentation.html):\n",
    "\n",
    "* ```sklearn.metrics.confusion_matrix()```\n",
    "* ```sklearn.metrics.accuracy_score()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "conf_mat_knn = None\n",
    "\n",
    "# accuracy\n",
    "acc_knn = None\n",
    "\n",
    "print ('Accuracy using kNN: {:.2f}%'.format(100.0*acc_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the following convenience function to visualize the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_mat, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix\n",
    "    \"\"\"\n",
    "    plt.imshow(conf_mat, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = conf_mat.max() / 2.\n",
    "    for i, j in itertools.product(range(conf_mat.shape[0]), range(conf_mat.shape[1])):\n",
    "        plt.text(j, i, conf_mat[i, j], horizontalalignment=\"center\",\n",
    "                 color=\"white\" if conf_mat[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the confusion matrix\n",
    "plot_confusion_matrix(conf_mat_knn, classes=noduleTypes,\n",
    "                      title='Confusion matrix: kNN as classifier (True label vs. Predicted label)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Question\n",
    "The confusion matrix highlights a problem of the kNN classifier as it is used now. Can you find it and explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Neural Networks\n",
    "Now that some kind of baseline result has been obtained with kNN, we can start developing a classifier based on neural networks.\n",
    "For this purpose, we will use the **Keras** and the **TensorFlow** library, which implements classes and functions that make building and training neural networks easy.\n",
    "Keras and TensorFlow use a bit of a special Python library, because it is based on symbolic representation of variables, which you may not be familiar with.\n",
    "For this reason, before we delve into the implementation of our neural network, we propose a short introduction to Keras and TensorFlow, which will clarify some of the doubts and questions you may have about these libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras / TensorFlow (short) introduction\n",
    "Keras is a Python library that lets you to define, optimize, and evaluate mathematical expressions, especially ones with multi-dimensional arrays (```numpy.ndarray```).\n",
    "\n",
    "Keras is not a programming language in the normal sense because you write a program in Python that builds expressions for TensorFlow. Still it is like a programming language in the sense that you have to\n",
    "*\tdeclare variables and give their types\n",
    "*\tbuild expressions for how to put those variables together\n",
    "*\tcompile expression graphs to functions in order to use them for computation.\n",
    "\n",
    "More about Keras: https://keras.io/\n",
    "\n",
    "TensorFlow is the backend used by Keras.\n",
    "\n",
    "More about TensorFlow: https://www.tensorflow.org/api_docs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a two-layer Neural Network to classify nodule features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figures/learning_framework.png\" alt=\"Learning framework\" align=\"right\" width=\"350\">\n",
    "\n",
    "In this first part of the assignment, we are going to build a neural network with one hidden layer in Keras. For this particular assignment we are going to use the fully connected layers, also called Dense layers in Keras (visit the following link to get more details of default parameters https://keras.io/layers/core/#dense).\n",
    "\n",
    "As we have seen in the lecture this week, in order to build our classification framework with neural networks, we have to define and specify parameters for three main components:\n",
    "\n",
    "1. NETWORK ARCHITECTURE\n",
    "2. LOSS FUNCTION\n",
    "3. OPTIMIZATION ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the neural network\n",
    "\n",
    "We will build this first network to have **one hidden layer of 10 neurons**.\n",
    "Later, we will experiment with a different number of neurons, hidden layers, but let's start with this one.\n",
    "\n",
    "Keep in mind that the size of the input and of the output layer of your network are given by the data and the classification problem you have to solve.\n",
    "Therefore, before you start building the network, it is good to check again the dimensionality of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = x_train.shape\n",
    "n_classes = len(noduleTypes)\n",
    "print (data_size)\n",
    "print (n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the architecture of a neural network with one hidden layer of 10 neurons.\n",
    "In your implementation, consider what follows:\n",
    "\n",
    "* Keras has two ways of defining network models, the **sequential** model and the **Model class**. These models are documented at this page: https://keras.io/models/about-keras-models/.\n",
    "\n",
    "* In this assignment, we will use a ```Sequential``` model, which can be seen as a **linear stack** of layers. This makes building a network easy, because you can simply initialize a model as ```Sequential```, and then add layers using the method ```add()```. You can find information about this at this link: https://keras.io/getting-started/sequential-model-guide/.\n",
    "\n",
    "* You can define your network model variable as ```network```, and return it as output of the function ```build_neural_network()```.\n",
    "\n",
    "* Use sigmoid linearity for the hidden layer (later in this assignment you will also be allowed to use ReLU!). In Keras, non linearities belong to the class ```Activation```.\n",
    "\n",
    "* Use softmax function as output of the network (also belong to the class ```Activation```)\n",
    "\n",
    "* Use a proper strategy to initialize the parameters of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network with 1 hidden layer of 10 neurons\n",
    "def build_neural_network(data_size, n_classes):\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the function that you have just coded to build your neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build neural network object\n",
    "network=build_neural_network(data_size, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are still not ready to use it, first you have to compile it, and in order to do that, you have to specify a couple of other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function & Optimization Algorithm\n",
    "\n",
    "Now that the architecture is defined, we have to specify the two other components of our learning framework, namely the loss function and the optimization algorithm.\n",
    "\n",
    "Once you have defined these two components, you will have to **compile** the network that you have defined with the function ```build_neural_network```, using the function ```network.compile()```.\n",
    "\n",
    "The ```network.compile()``` function requires the following input parameters:\n",
    "1. loss function -> the loss function\n",
    "2. optimizer -> the optimization algorithm\n",
    "3. metrics -> the performance parameters you want to compute\n",
    "\n",
    "You can find information about how to use the ```compile()``` function at this page: https://keras.io/getting-started/sequential-model-guide/.\n",
    "\n",
    "### Loss\n",
    "We have to define a function that, given the network, gets the predicted probability for a given input sample.\n",
    "Since we are dealing with a multi-class classification problem, **categorical cross-entropy** seems a reasonable choice.\n",
    "\n",
    "### Optimization algorithm\n",
    "We also have to specify how we want to train our model. In our case, we will use \"Stochastic Gradient Descent\". As we have seen in the lecture this week, gradient descent algorithms need a **learning rate**, which indicates how much we step in the (opposite) direction of the gradient. We have also seen that strategy to adapt the learning rate during training are possible, but for the moment we just define a fixed learning rate. Pick a value and see what happens, you can optimize this later.\n",
    "\n",
    "### Metrics\n",
    "Since we are developing a classifier for a multi-class problem, the accuracy seems like a reasonable choice.\n",
    "\n",
    "In the end, you need to compile your network with your settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace None with your code ###\n",
    "\n",
    "loss = None # define the (string) loss function\n",
    "learning_rate = None # pick a value for your learning rate\n",
    "sgd = None # define Stochastic Gradient Descent as the keras optimizer, which takes the learning rate as input parameter\n",
    "metrics = None # define (Python) list of metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to compile your network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(loss=loss, optimizer=sgd, metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using categorical cross-entropy as loss function, we need a representation of labels in the training (and later validation) data in a 'one-hot' form. This means that if we have 5 classes, the format of labels has to be the following:\n",
    "\n",
    "* ```y_train = 1 -> [1, 0, 0, 0, 0]```\n",
    "* ```y_train = 2 -> [0, 1, 0, 0, 0]```\n",
    "* ```y_train = 3 -> [0, 0, 1, 0, 0]```\n",
    "* ```y_train = 4 -> [0, 0, 0, 1, 0]```\n",
    "* ```y_train = 5 -> [0, 0, 0, 0, 1]```\n",
    "\n",
    "Lucky for you, Keras has implemented such a [function](https://keras.io/utils/#to_categorical). We can use that to convert the given format into a 'one-hot' format. First, check the format of labels in your dataset, then check if the function of Keras does what it is supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Replace None with your code ###\n",
    "\n",
    "y_train_one_hot = None\n",
    "\n",
    "y_validation_one_hot = None \n",
    "\n",
    "# check number of samples per class\n",
    "print (np.sum(y_train_one_hot, axis=0))\n",
    "print (np.sum(y_validation_one_hot, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check validation performance before training\n",
    "All the main components required to train our network have been defined now.\n",
    "However, we have seen that in order to properly monitor the behaviour of a network during training, we should check the performance (the accuracy) on a separate validation set, and see if it returns something sensible.\n",
    "For this purpose, you can use the function ```network.evaluate``` in Keras, and set the ```batch_size``` to 1, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scores=network.evaluate(x_validation, y_validation_one_hot, batch_size=1)\n",
    "val_loss=scores[0]\n",
    "val_acc=scores[1]\n",
    "print ('Initial validation accuracy = {:.2f}%'.format(100.*val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning procedure\n",
    "Now we can write the learning algorithm, as we have seen in the lecture this week.\n",
    "Basically, we will iteratively update the parameters of our network by extracting mini-batches from the training set, until all the training samples have been used. After a complete round, one epoch is done. We repeat this procedure for a number of epochs that you define.\n",
    "During the training loop, we also want to check the performance of the trained network on the validation set.\n",
    "Therefore, for each epoch, after a training pass, we also classify the validation set.\n",
    "\n",
    "We provide the main structure of the learning script, implement the missing parts:\n",
    "\n",
    "* Train the network\n",
    "* Get the training loss and accuracy\n",
    "* Validate your network on the validation data\n",
    "* Get the validation loss and accuracy\n",
    "\n",
    "During training, we will also be saving to disk the parameters of the network wich has the best performance on the validation set. This will be stored as the file ```best_model.h5``` in the direcotry ```network_dir```, which by default is the root directory of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Replace None with your code ###\n",
    "\n",
    "n_epoch = None\n",
    "batch_size = None\n",
    "network_filepath = os.path.join(network_dir, 'best_model.h5')\n",
    "\n",
    "# lists where we will be storing values during training, for visualization purposes\n",
    "tra_losses = [] # list for training loss\n",
    "tra_accs = [] # list for training accuracy\n",
    "val_losses = [] # list for validation loss\n",
    "val_accs = [] # list for validation accuracy\n",
    "\n",
    "# we want to save the parameters that give the best performance on the validation set\n",
    "# therefore, we store the best validation accuracy, and save the parameters to disk\n",
    "best_validation_accuracy = 0 # best validation accuracy\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    st = time.time()\n",
    "    \n",
    "    # Train your network\n",
    "    results = network.fit(None)\n",
    "    \n",
    "    # Get training loss and accuracy\n",
    "    training_loss = results.history[None]\n",
    "    training_accuracy = results.history[None]\n",
    "    \n",
    "    # Add to list\n",
    "    tra_losses.append(training_loss)\n",
    "    tra_accs.append(training_accuracy)\n",
    "    \n",
    "    # Evaluate performance (loss and accuracy) on validation set\n",
    "    scores = network.evaluate(None)     \n",
    "    validation_loss = scores[None]\n",
    "    validation_accuracy = scores[None]\n",
    "    \n",
    "    # Add to list\n",
    "    val_losses.append(validation_loss)\n",
    "    val_accs.append(validation_accuracy)\n",
    "    \n",
    "    # (Possibly) update best validation accuracy and save the network\n",
    "    if validation_accuracy > best_validation_accuracy:\n",
    "        best_validation_accuracy = None\n",
    "        network.save(network_filepath)\n",
    "    \n",
    "    # Visualization of the learning curves\n",
    "    fig = plt.figure(figsize=(10, 5))\n",
    "    tra_loss_plt, = plt.plot(range(len(tra_losses)), tra_losses, 'b')\n",
    "    tra_accs_plt, = plt.plot(range(len(tra_accs)), tra_accs, 'c')\n",
    "    val_loss_plt, = plt.plot(range(len(val_losses)), val_losses, 'g')\n",
    "    val_acc_plt, = plt.plot(range(len(val_accs)), val_accs, 'r')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend([tra_loss_plt, tra_accs_plt, val_loss_plt, val_acc_plt], \n",
    "              ['training loss', 'training accuracy', 'validation loss', 'validation accuracy'],\n",
    "              loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.title('Best validation accuracy = {:.2f}%'.format(100. * best_validation_accuracy))\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: validation set\n",
    "Now we can use the trained network to classify the validation set (again), and check that the performance corresponds to the best value obtained during training. We can compute the accuracy and also visualize the confusion matrix, to get a feeling how well we are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the model found for the best accuracy\n",
    "# the function 'load_model' takes care of compiling again the function \n",
    "best_network = keras.models.load_model(network_filepath)\n",
    "\n",
    "# Calculate the prediction on the validation set using the best network\n",
    "prediction = best_network.predict(None)\n",
    "y_validation_auto = np.argmax(None)\n",
    "\n",
    "# Assess the confusion matrix to check if the performances corresponds to the best value obtained during the training\n",
    "conf_mat_nn = None \n",
    "acc_nn = None\n",
    "print('Accuracy on validation set: {:.2f}%'.format(100. * acc_nn))\n",
    "plot_confusion_matrix(conf_mat_nn, classes=noduleTypes,\n",
    "                      title='Confusion matrix: Neural Network classifier (True label vs. Predicted label)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the labels in the validation set to identify the cases that you are misclassifying, and see what the network says about those cases. Since the labels in our dataset are given by humans (no ground truth available, only reference standard), there can be some confusion in the way nodules are classified, even in the reference standard.\n",
    "\n",
    "#### Question\n",
    "Based on what you have learned about the appearnce of nodules at the beginning of this notebook, do you think you agree with the labels predicted by your network? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your answer here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: test set\n",
    "Now we can repeat the classification step on the test set, and submit the results to grand-challenge.org. During the test procedure, we will save the predictions in a csv file, which you can submit.\n",
    "Please note that the reference standard in grand-challenge has labels y = [1, ..., 5]. Take this into account when making the csv file for your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the test set\n",
    "\n",
    "# Create a .csv file for saving the results of the classification\n",
    "h_csv = open('./results.csv', 'w')\n",
    "h_csv.write('nodule_id,label\\n')\n",
    "\n",
    "# For each samples within the test set, get the prediction from the network and save it \n",
    "n_test_samples = x_test.shape[0]\n",
    "for n in range (n_test_samples):\n",
    "    \n",
    "    # get one test sample\n",
    "    test_sample = x_test[n,:].reshape(1,256).astype(np.float32)\n",
    "    \n",
    "    # get its nodule is\n",
    "    nodule_id = nodule_ids_test[n]\n",
    "    \n",
    "    # make prediction using neural network\n",
    "    prediction = best_network.predict(test_samples)\n",
    "    \n",
    "    # convert prediction into a label\n",
    "    y = np.argmax(None) # <--- replace None with your code!\n",
    "    \n",
    "    # write the label to file\n",
    "    h_csv.write('{},{}\\n'.format(nodule_id.decode('UTF-8'), y+1))\n",
    "\n",
    "    # close file\n",
    "    h_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit your results!\n",
    "\n",
    "You can now download the results file with this link: [results.csv](results.csv).  \n",
    "Next, upload your result to the challenge website (https://ismi-nodules.grand-challenge.org/) and see how well you performed compared to your fellow students! You can submit as often as you want, only the best result counts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Improve your neural network\n",
    "So far, we have implemented a simple network with one hidden layer, 10 neurons and sigmoid activation function.\n",
    "We also used a constant learning rate.\n",
    "This means that there is plenty of room for improvement!\n",
    "You can, for example:\n",
    "* change the architecture of your network, add neurons, add layers\n",
    "* change the activation function, try ReLU and see what happens\n",
    "* change the learning rate, or try to find a strategy to adapt it during training\n",
    "\n",
    "In order to fine-tune these parameters, you may want to expand the main script used for training and validation to include a search for the optimal set of hyper-parameters (for example, cross-validation).\n",
    "For each experiment:\n",
    "* provide a clear description of the setup (value, range of parameters used in the search), which we can read and understand\n",
    "* save the trained network, which you can load later\n",
    "\n",
    "In particular, show and explain:\n",
    "* how the results change by changing the learning parameters/architecture of the network\n",
    "* how the learning rate worked and how it affected the performance of the neural network\n",
    "\n",
    "Note that hyper-parameters tuning has to be done using the **validation** set.\n",
    "When you are happy with the performance on the vaidation set, you can submit the results to grand-challenge!\n",
    "\n",
    "**This may sound like a boring task, but is actually make it takes to make good neural networks!!! Engineers and researchers do this very often!!!**\n",
    "\n",
    "Do not modify previous cells in this notebook, but please add new cells and copy-paste your code, and modify it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define improved neural network\n",
    "def improved_neural_network(data_size, n_classes):\n",
    "    # Your code here:\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Learning-rate\n",
    "# * Optimizer (SDG)\n",
    "# * Loss\n",
    "# * Compile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train improved network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Train a neural network with raw nodule data\n",
    "Now that you have developed your supervised learning framework using the features that we provided, repeat the procedure using raw data as input. You can use the functions provided at the beginning of this notebook to extract 2D views from 3D nodules, which could be useful to develop your network.\n",
    "\n",
    "Once patches have been extracted, you can use a strategy similar to what showed in the lecture this week (when a linear model was used to classify CIFAR10 images) and vectorize patches to obtain feature vectors.\n",
    "\n",
    "Repeat the training procedure, tune the hyper-parameters, and submit the new results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that loads and preprocesses the raw data.\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network with raw nodule data\n",
    "def raw_neural_network(data_size, n_classes):\n",
    "    # Your code here:\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your own training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching Assistants:\n",
    "\n",
    "Send us an email for questions. Remember to send your assignment before Monday midnight.\n",
    "\n",
    "- Ecem Lago: ecem.lago@radboudumc.nl\n",
    "- Thomas De Bel: Thomas.deBel@radboudumc.nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
