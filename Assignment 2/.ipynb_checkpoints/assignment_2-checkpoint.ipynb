{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vessel segmentation in retina fundus images\n",
    "<img src=\"images/21_training.png\" width=\"250\" height=\"250\" align=\"right\">\n",
    "The goal of this assignment is to develop an algorithm for automatic segmentation of vessels in retina fundus images using \n",
    "feature classification and morphological filtering.\n",
    "\n",
    "\n",
    "## Teaching assistants\n",
    "\n",
    "- Hans Pinckaers: hans.pinckaers@radboudumc.nl\n",
    "- Kevin Koschmieder: kevin.koschmieder@radboudumc.nl\n",
    "\n",
    "## Students\n",
    "Please fill in this cell with your name and e-mail address. This information will be used to grade your assignment.\n",
    "\n",
    "* Lisette Boeijenk, e.boiejenk@student.ru.nl\n",
    "* Stephan Dooper, s.dooper@student.ru.nl\n",
    "* Klaus Lux, k.lux@student.ru.nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "* Groups: You should work in group or alone. Working in groups of 2-3 is preferable.\n",
    "* Deadline for this assignment: \n",
    " * Monday February 11th at 23:59h.\n",
    " * 5 points (maximum grade = 100 points) penalization per day after deadline\n",
    "* Submit your **fully executed** notebook to grand-challenge.org (more details about this later in this notebook)\n",
    "* The file name of the notebook you submit must be ```NameSurname1_NameSurname2_NameSurname3.ipynb```\n",
    "\n",
    "This notebooks contains cells with snippets of code that we provide in order to load and visualize data, but also some convenience functions that could be useful to develop your assignment. We also provide templates for functions that have to be implemented, with a given list of input variables and some output variables. **Feel free to modify the input and output variables to adapt them to your favourite implementation.** However, this should at least provide the outputs required to develop the rest of the notebook.\n",
    "\n",
    "Your submission should contain the **fully executed** notebook with **your code** implemented, as well as **your answers** to questions, which will be used to grade your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical background\n",
    "\n",
    "Retinal vessel segmentation and delineation of morphological attributes of retinal blood vessels, such as length, width, tortuosity, branching patterns and angles are utilized for the diagnosis, screening, treatment, and evaluation of various cardiovascular and ophthalmologic diseases such as diabetes, hypertension, arteriosclerosis and chorodial neovascularization.\n",
    "\n",
    "Automatic detection and analysis of the vasculature can assist in the implementation of screening programs for diabetic retinopathy, can aid research on the relationship between vessel tortuosity and hypertensive retinopathy, vessel diameter measurement in relation with diagnosis of hypertension, and computer-assisted laser surgery.\n",
    "\n",
    "Automatic generation of retinal maps and extraction of branch points have been used for temporal or multimodal image registration, retinal image mosaic synthesis. Moreover, the retinal vascular tree is found to be unique for each individual and can be used for biometric identification.\n",
    "\n",
    "## Data \n",
    "\n",
    "In this assignment, we are going to develop a system to automatically **segment vessels** in human retina fundus images. For this purpose, we are going to use data from the publicly available DRIVE dataset (http://www.isi.uu.nl/Research/Databases/DRIVE/).\n",
    "The DRIVE dataset consists of 40 images, 20 used for training and 20 used for testing. Each case contains:\n",
    "* a fundus (RGB) image\n",
    "* a binary mask, which indicates the area of the image that has to be analyzed (removing black background)\n",
    "* manual annotations of retinal vessels, provided as a binary map\n",
    "\n",
    "## Tasks for this assignment\n",
    "<img src=\"images/submission.png\" width=\"500\" align=\"right\" style=\"padding-left:20px\">\n",
    "1. Develop a system to segment vessels in retinal images in this notebook. You will have to submit this notebook with your code, which we will run and evaluate, together with the results of the segmentation.\n",
    "2. Use the training set provided with the DRIVE dataset to train/tune the parameters of your system. You cannot use data from the test set available on the DRIVE website, nor from other datasets. \n",
    "3. Apply it to the test dataset and generate a binary map of the segmented vessel. The map must have the same size as the input image.\n",
    "4. Submit the results of your algorithm to the mini-challenge framework. You can create a zip-file by running the corresponding cell at the end of the notebook. You will have to **submit** this file to the mini-challenge framework (https://drive.grand-challenge.org/). In order to submit results, click on the **Submit** tab on grand-challenge, then click on the **Choose File** button to select your .zip \"Predictions File\" and submit it. You can monitor the execution of your submission by clicking on the link at the end of the page. Note that while grading your assignment, we will run your implementation and reproduce your results. Any significant discrepancy between the results submitted to the mini-challenge framework and the one computed using this notebook will be penalized and discussed with the student. **Note**: It is allowed to submit multiple times, checking the performance of different results of your algorithm. In order to keep track of your submissions, you can add a comment when you submit your .zip file.\n",
    "5. Once you have completed your assignment, submit this notebook via grand-challenge.org, by clicking on the \"Jupyter Notebook\" submission button. \n",
    "\n",
    "<font color='red'>\n",
    " <h3> Only submissions of teams will be accepted, and single users should not submit (unless they don't have a team). <br/><br/>\n",
    "\n",
    " The name of the team should begin with \"ismi19\".<br/><br/>\n",
    " \n",
    "   Only upload the fully-executed(!) notebook in your last submission\n",
    "</h3>\n",
    "</font>\n",
    "\n",
    "## Implementation \n",
    "\n",
    "### First task: Max 60 pts\n",
    "You will be asked to implement your vessel segmentation system based on **pixel classification based on texture analysis**.\n",
    "For this purpose, you can use the following publications as reference:\n",
    "\n",
    "* [1] M. Niemeijer et al. \"Comparative study of retinal vessel segmentation methods on a new publicly available database\" (the paper suggested last week)\n",
    "* [2] Y. Yang et al., \"An automatic hybrid method for retinal blood vessel extraction\"\n",
    "* [3] M.M. Fraz et al., \"Blood vessel segmentation methodologies in retinal images - A survey\"\n",
    "\n",
    "The first paper explains how to segment retinal vessels using pixel classification.\n",
    "In this assignment, you should implement the solution based on pixel classification as is explained in section 2.3.5 of the first paper. \n",
    "\n",
    "### Second task: Max 40pts\n",
    "Next, try to **improve the segmentation** result. The goal is to design the best CAD system for vessel segmentation and validate its performance on the DRIVE dataset! A few ideas to improve the system are:\n",
    "\n",
    "* More features (Directional filters, local binary patterns, Gabor filters, etc.). Get creative!\n",
    "* Postprocessing to improve the results using morphological filtering\n",
    "* More training samples\n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "A method based on texture analysis and pixel classification, requires a set of labeled samples, for which the correct label is known. We call this a *training set*. In order to build a training set, positive (vessel) and negative (non-vessel) samples have to be extracted from the images in the training set. This set will be used to classify new (test) samples, using for example a **nearest-neighbour** strategy for assigning labels. To objectively validate different methods, we will compare the results of your method with reference data (manual annotations). The outline of the approach you are about the implement is:\n",
    "1. Load data: download the data, import and convert to numpy, inspect the available annotations.\n",
    "2. Feature extraction: Implement texture analysis to extract features from the images and create a training set.\n",
    "3. Train the classifier: for example a nearest-neighbour classifier.\n",
    "4. Test the classifier: apply the classifier to new data to obtain a new vessel map.\n",
    "5. Submit your results: submit the output of your algorithm to grand-challenge, where it will be evaluated and compared to other submissions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load data\n",
    "\n",
    "Let's get started by importing libraries needed for this assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download the training data (images, masks and labels) and load it into memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Script to download the dataset to your local computer\n",
    "import requests\n",
    "from tqdm import tqdm_notebook\n",
    "import zipfile\n",
    "link = 'https://surfdrive.surf.nl/files/index.php/s/Kn4hCF4G919ijr3/download'\n",
    "file_name = \"DRIVE.zip\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in tqdm_notebook(response.iter_content(chunk_size=4096), desc='Downloading data'):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "with zipfile.ZipFile(file_name,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "os.remove(file_name)\n",
    "data_folder = 'DRIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is on the machine you are using, you can continue working on this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T15:43:16.602408Z",
     "start_time": "2019-02-01T15:43:16.150110Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_file_list(path, ext=''):\n",
    "    return sorted([os.path.join(path, f) for f in os.listdir(path) if f.endswith(ext)])\n",
    "\n",
    "def load_img(path):\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "train_img_files = get_file_list(os.path.join(data_folder, 'training', 'images'), 'tif')\n",
    "train_msk_files = get_file_list(os.path.join(data_folder, 'training', 'mask'), 'gif')\n",
    "train_lbl_files = get_file_list(os.path.join(data_folder, 'training', '1st_manual'), 'gif')\n",
    "\n",
    "train_imgs = [load_img(f) for f in train_img_files]\n",
    "train_msks = [load_img(f) for f in train_msk_files]\n",
    "train_lbls = [load_img(f) for f in train_lbl_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to visualize (1) the fundus image, (2) the binary mask, (3) the manual annotation. Notice the use of the argument ```cmap``` to change the ```colormap``` of the visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T15:43:18.039691Z",
     "start_time": "2019-02-01T15:43:18.010351Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def show_image(img, msk, lbl):\n",
    "    matplotlib.rcParams['figure.figsize'] = (20, 12)\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img)\n",
    "    plt.title('RGB image')\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(msk, cmap='gray')\n",
    "    plt.title('Mask')\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(lbl, cmap='gray')\n",
    "    plt.title('Manual annotation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some images in the training set using the function defined above. Try setting a few values for ```i```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "i = None # Try some values here    \n",
    "show_image(train_imgs[i], train_msks[i], train_lbls[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Feature extraction\n",
    "\n",
    "### Create a Gaussian filter kernel\n",
    "\n",
    "The system described in paper [1] uses filters based on Gaussian and derivative of Gaussian functions. Now we will create a two-dimensional Gaussian filter kernel that can be used the extract features from an image. \n",
    "The part of code that you will implement to create the Gaussian kernel is in the next cell, followed by a piece of code to visualize the Gaussian kernel.\n",
    "\n",
    "**Note**. If you completed the first assignment, you have implemented this function already. You can reuse the one you defined in the first assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_kernel_2d(sigma, x0=0, y0=0, kernel_size=3):\n",
    "    '''\n",
    "        Computes a 2D Gaussian kernel with a given sigma (in pixels), centered at x0, y0\n",
    "        The size of the kernel can be set as a multiple of the sigma.\n",
    "        Kernel size should be a multiple of sigma\n",
    "    '''\n",
    "    # >>> YOUR CODE STARTS HERE <<<\n",
    "    # g = None\n",
    "    # >>> YOUR CODE ENDS HERE <<<  \n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def visualize_gaussian_kernel(gaussian_kernel):\n",
    "    ''' \n",
    "        Visualizes a Gaussian kernel\n",
    "    '''    \n",
    "    fig = plt.figure()\n",
    "    ax = Axes3D(fig)\n",
    "    y_dim, x_dim = gaussian_kernel.shape\n",
    "    sx = x_dim // 2\n",
    "    sy = y_dim // 2\n",
    "    x, y = np.meshgrid(range(-sx, sx + 1), range(-sy, sy + 1))\n",
    "    ax.plot_surface(x, y, gaussian_kernel, rstride=1, cstride=1, cmap='jet')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will help you create the Gaussian filter kernel by visualizing it in 3D. \n",
    "It will show the Gaussian filter kernel you defined in the function above.\n",
    "Try changing kernel_size to see the kernel at different scales. With `kernel_size=3*sigma` you should be able to see the 'interesting' part of the gaussian function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "sigma = None # Try some values here\n",
    "kernel_size = None\n",
    "gaussian_kernel = gaussian_kernel_2d(sigma, x0=0, y0=0, kernel_size=kernel_size)\n",
    "visualize_gaussian_kernel(gaussian_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-04T15:44:08.850431Z",
     "start_time": "2019-02-04T15:44:08.837181Z"
    }
   },
   "source": [
    "It should look roughly like this (image from the first assignment): \n",
    "<img src=\"../assignment_1/images/gaussian.gif\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect features\n",
    "\n",
    "The next function you will implement is a *feature extractor* that will extract features from an image. You will have to decide what filters, or texture analysis operators, will be used to describe the appearance of pixels. \n",
    "\n",
    "In order to define the number of features, consider the approach that you are going to implement. If based on paper [1], several filters based on a Gaussian kernel will be used, at several scales (different sigma values). \n",
    "\n",
    "Suggested features: \n",
    "- Gaussians at different scales\n",
    "- Derivatives of Gaussians at different scales and angles\n",
    "- Second derivatives at different scales and angles\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your feature extraction function here:\n",
    "\n",
    "Suggested functions to use\n",
    " - ```gaussian_kernel_2d``` (your own implementation!)\n",
    " - ```np.gradient```\n",
    " - ```scipy.signal.fftconvolve```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(img):\n",
    "    \"\"\"\n",
    "        Computes features from a give input image.\n",
    "        returns a Python list of feature maps (numpy arrays, the same size as the image)\n",
    "        that contain the result of the convolution of each filter with the input image.\n",
    "        \n",
    "        As mentioned in the lecture, given the input image 'img', only the green channel\n",
    "        is typically used in retina fundus image analysis. Therefore, the green channel\n",
    "        is extracted, and you can use it as input for all subsequent steps in this function.\n",
    "    \"\"\"    \n",
    "    img_green = img[:,:,1]\n",
    "    # the first feature element is the green channel itself    \n",
    "    features = [img_green]\n",
    "    \n",
    "    # >>> YOUR CODE STARTS HERE <<<\n",
    "    # \n",
    "    #    features.append(my_feature_1)\n",
    "    #    features.append(my_feature_2)\n",
    "    #    ...etc\n",
    "    #\n",
    "    # >>> YOUR CODE ENDS HERE <<<  \n",
    "        \n",
    "   \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the output of your feature extraction function here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = None # select the image in the training set\n",
    "img = train_imgs[i]\n",
    "features = extract_features(img)\n",
    "matplotlib.rcParams['figure.figsize'] = (4, 4)\n",
    "for f in features:\n",
    "    plt.imshow(f, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue\">Q: How many features does your `extract_features` function generate? Explain where these features come from.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green\">A: Your answer here</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training data\n",
    "\n",
    "We have read images, masks and manual annotations from the training set already:\n",
    "* train_imgs (images)\n",
    "* train_msks (masks)\n",
    "* train_lbls (labels)\n",
    "\n",
    "Now, we initialize the numpy arrays that will contain training **samples (x)** and corresponding training **labels (y)**. The matrix `x_train` will contain the features of each pixel in the training set, one sample per row, and one feature per column. The vector `y_train` will contain the corresponding label of each training sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "''' \n",
    "Pick a reasonable number for n_samples_per_class_per_image. \n",
    "More samples will likely result in better performance, but may make your method slow\n",
    "10 samples per class per image is probably the minimum you need for some indicative results\n",
    "if you want state of the art, you can take up to 2000 or more samples, \n",
    "but beware that this may result in long processing times\n",
    "'''\n",
    "n_samples_per_class_per_image = None\n",
    "n_classes = None # How many classes?\n",
    "n_features = None # How many features?\n",
    "\n",
    "assert n_samples_per_class_per_image is not None\n",
    "assert n_classes is not None\n",
    "assert n_features is not None\n",
    "\n",
    "vector_size = n_classes * n_samples_per_class_per_image * len(train_imgs)\n",
    "\n",
    "# x_train will contain the vector of features, extracted at <vector_size> locations in the training set\n",
    "x_train = np.zeros((vector_size, n_features))\n",
    "# y_train will contain the corresponding labels\n",
    "y_train = np.zeros(vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is useful when you want to collect samples randomly selected from a given binary mask, for example when you want to obtain positive and negative samples from a segmentation map. In the case of vessel segmentation, this function can be used to obtain a given number of coordinates that contain pixels labeled as vessel or non-vessel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def get_random_indices(array, n):\n",
    "    '''\n",
    "    returns n indices (as a tuple of x-coords and y-coords) where the array is True\n",
    "    '''\n",
    "    x, y = np.where(array)\n",
    "    ixs = np.random.choice(len(x), size=n, replace=False)\n",
    "    return x[ixs], y[ixs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following piece of code will loop through the images in the training set and extract features using the feature extractor you implemented. Next it will pick random locations (using the function above) in the image for background pixels and foreground pixels. \n",
    "Using these locations and the produced features you can now fill the numpy arrays `x_train` and `y_train` with a set of training samples (x) and corresponding training labels (y) to be used for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint: think of features as a [height x width x features] matrix, so every x,y location contains a vector of features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, (img, lbl, msk) in enumerate(zip(train_imgs, train_lbls, train_msks)):\n",
    "    print('extracting features', i)\n",
    "            \n",
    "    # extract features from the given image\n",
    "    # implement the extract_features function defined above!\n",
    "    features = extract_features(img) \n",
    "    features = np.moveaxis(np.array(features), 0, 2)\n",
    "    \n",
    "    # >>> YOUR CODE STARTS HERE <<<\n",
    "    \n",
    "    # make two boolean arrays, the same size as the image\n",
    "    # positive_samples should be True at vessel pixels\n",
    "    # negative_samples should be True at background pixels (but not outside the mask!)\n",
    "    # You can use the numpy element wise logical operations \n",
    "    positive_samples = None\n",
    "    negative_samples = None \n",
    "    \n",
    "    # >>> YOUR CODE ENDS HERE <<<\n",
    "    \n",
    "    # extract indices for our set of samples\n",
    "    p_idx = get_random_indices(positive_samples, n_samples_per_class_per_image)    \n",
    "    n_idx = get_random_indices(negative_samples, n_samples_per_class_per_image) \n",
    "    \n",
    "    #>>> YOUR CODE STARTS HERE <<<\n",
    "    \n",
    "    # TODO         \n",
    "    # extract positive and negative samples from the produced features and place them in x_train\n",
    "    # create labels and place them in y_train \n",
    "    # x_train[?:?] = ?\n",
    "    # y_train[?:?] = ?\n",
    "    \n",
    "    # >>> YOUR CODE ENDS HERE <<<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue\">Q: Do you think you have to modify the range of the values of the features in the training set, and why?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green\">A: your answer here</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples obtained using the code above need to be normalized to obtain good performance. \n",
    "Implement a normalization function that will make sure the training dataset has zero mean and unit variance in the cell below.\n",
    "The function should also return the mean and standard deviation of your training dataset. **Note**: a division by ```zero``` may cause some numerical problems..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def normalization(x_train):\n",
    "    '''\n",
    "        apply normalization to the samples (x_train)\n",
    "        returns a new array that has been properly normalized, \n",
    "        also returns the mean and standard deviation per feature in the training set \n",
    "    ''' \n",
    "    # >>> YOUR CODE STARTS HERE <<<\n",
    "    \n",
    "    x_train_normalized = None # array of normalized features\n",
    "    mean = None # array of mean value per feature \n",
    "    std = None # array of std value per feature\n",
    "    \n",
    "    # >>> YOUR CODE ENDS HERE <<<\n",
    "    \n",
    "    return x_train_normalized, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "x_train_normalized, train_mean, train_std = normalization(x_train)\n",
    "\n",
    "# check if the data is properly normalized!\n",
    "assert np.all(np.abs(x_train_normalized.mean(axis=0)) < 1e-9), 'Mean should be zero!'\n",
    "assert np.all(np.abs(x_train_normalized.std(axis=0) - 1) < 1e-9), 'Std should be one!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train the classifier\n",
    "Now that you have created the training data we can use it to train our classifier!\n",
    "The classifier that we have seen in the lecture is k-Nearest Neighbors (kNN).\n",
    "You can use that one, which is available in the ```sklearn``` library (imported at the beginning of this notebook).\n",
    "Pick a meaningful value for ```k``` (the number of neighbors).\n",
    "\n",
    "You will notice that kNN is a **slow** algorithm when you apply it to new data.\n",
    "We propose using it because it is the only one we have seen so far.\n",
    "But if you know other classifiers thath you want to use in this assignment, feel free to do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# pick the value for k\n",
    "n_neighbors = None\n",
    "\n",
    "assert n_neighbors is not None\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=n_neighbors) \n",
    "classifier.fit(x_train_normalized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Test the classifier\n",
    "Now that we have a trained classifier we can apply it to some unseen data and evaluate its performance.\n",
    "First we load the test data (note that labels for test data are not available now!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "test_img_files = get_file_list(os.path.join(data_folder, 'test', 'images'), 'tif')\n",
    "test_msk_files = get_file_list(os.path.join(data_folder, 'test', 'mask'), 'gif')\n",
    "\n",
    "test_imgs = [load_img(f) for f in test_img_files]\n",
    "test_msks = [load_img(f) for f in test_msk_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue\">Q: In the next cell we define the function to normalize the test data. The function requires a mean and standard deviation as input, what should you use as input for these?</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:green\">\n",
    "A: Your answer here</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the mean and standard deviation to the correct values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "test_mean = None\n",
    "test_std = None\n",
    "\n",
    "assert test_mean is not None\n",
    "assert test_std is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def normalization_test(x_test):\n",
    "    '''\n",
    "    Normalization of the test data\n",
    "    ''' \n",
    "    return (x_test - test_mean) / test_std "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function will classify a new, unseen image using our classifier. It will:\n",
    "\n",
    "* Extract features for every pixel in the image\n",
    "* Apply normalization\n",
    "* Classify every pixel\n",
    "* Put the pixels back in the shape of the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def classify_img(classifier, img, msk, threshold):\n",
    "\n",
    "    # compute features\n",
    "    print('extraction features')\n",
    "    features = extract_features(img) \n",
    "    print('features extracted')\n",
    "\n",
    "    h, w, c = img.shape\n",
    "    # reshape the features to an array of feature vectors\n",
    "    x_test = np.moveaxis(np.array(features), 0, 2).reshape((h * w, -1))\n",
    "    # normalize the features\n",
    "    x_test_normalized = normalization_test(x_test)\n",
    "\n",
    "    print('classifying pixels with nearest-neighbor')\n",
    "    p_test = classifier.predict_proba(x_test_normalized)\n",
    "    print('classification done')\n",
    "\n",
    "    # reshape to image format, set pixels outside the mast to zero\n",
    "    p_test_reshaped = p_test[:,1].reshape(h, w) * msk\n",
    "\n",
    "    # Threshold the probabilitymap to obtain the final result\n",
    "    final_output = (p_test_reshaped > threshold) * 255 \n",
    "    \n",
    "    # return the raw output and the thresholded image\n",
    "    return p_test_reshaped, final_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop over all the images in the test set and do the following for every image:\n",
    "* classify all pixels, using the above defined `classify_img` function\n",
    "* plot the results\n",
    "* save the thresholded output image as a png file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# First, we will set the folder where the results will be saved\n",
    "# feel free to modify this location (if you wish not to overwrite previous results)\n",
    "result_output_folder = 'vessel_segmentation_results'\n",
    "\n",
    "if not(os.path.exists(result_output_folder)):\n",
    "    os.makedirs(result_output_folder)\n",
    "\n",
    "# Now loop over all test images\n",
    "for i, (img, msk) in enumerate(zip(test_imgs, test_msks)):\n",
    "    print('processing image:', i)\n",
    "    raw_output, final_output = classify_img(classifier, img, msk, optimal_threshold)\n",
    "\n",
    "    # plot the results\n",
    "    matplotlib.rcParams['figure.figsize'] = (10, 6)\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(raw_output)\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(final_output)\n",
    "    plt.show()\n",
    "\n",
    "    # save the image as png file\n",
    "    im = Image.fromarray(final_output.astype('uint8'))\n",
    "    im.save(os.path.join(result_output_folder, \"{}.png\".format(i + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"color:blue\">Q: What is the output of your kNN classifier? Did you obtain a label for each pixel? How is that label computed?</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p style=\"color:green\">A: Your answer here</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Submit your results!\n",
    "\n",
    "After processing all the images in the test set you can execute the cell below to create a zip-file of the images with the segmentation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "shutil.make_archive('results', 'zip', result_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now download this zipfile with this link: [results.zip](results.zip).  \n",
    "Next, upload your result to the challenge website (https://drive.grand-challenge.org/evaluation/submissions/create/) and see how well you performed compared to your fellow students! You can submit as often as you want, only the best result counts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second task\n",
    "### Improve your results!\n",
    "\n",
    "Try to improve your results and resubmit.\n",
    "\n",
    "A few ideas to improve the system are:\n",
    "\n",
    "* Change the threshold\n",
    "* More training samples\n",
    "* Different classifier (if you know any)\n",
    "* More features (Directional filters, local binary patterns, Gabor filters, get creative!)\n",
    "* Postprocessing to improve the results (morphological operations), for example remove the rim around the field of view (which is obviously not part of a vessel), remove noise, etc.\n",
    "\n",
    "## DO NOT MODIFY THE CELLS ABOVE! INSTEAD, MAKE NEW CELLS BELOW AND REUSE CODE FROM PREVIOUS CELLS!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# You can add your new code here.\n",
    "# Add more cells using the \"+\" button in the toolbar of this notebook.\n",
    "# If you want to add \"code\" cells, select the \"Code\" type (default).\n",
    "# If you want to add \"text\" cells, select the \"Markdown\" type."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
