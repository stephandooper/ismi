{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Lungs detection in chest X-ray "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "<img src=\"figures/xray.jpeg\" width=\"200\" height=\"200\" align=\"right\">\n",
    "In this assignment, you are going to develop a system to automatically detect the bounding boxes surrounding the lungs in a chest X-ray scan.\n",
    "For this task we are going to use a single-shot network called [Yolo](https://arxiv.org/pdf/1506.02640.pdf) (You Only Look Once). Details on the YOLO (and the more recent YOLOv2/YOLO9000) method can be found in the following papers:\n",
    "\n",
    "  1. **You Only Look Once: Unified, Real-Time Object Detection** [https://arxiv.org/abs/1506.02640]\n",
    "  2. **YOLO9000: Better, Faster, Stronger** [https://arxiv.org/abs/1612.08242] \n",
    "\n",
    "Chest X-ray is the most commonly acquired image in medicine. Chest X-ray uses a very small dose of ionizing radiation to produce pictures of the inside of the chest. It is used to evaluate the lungs, heart and chest wall and may be used to help diagnose shortness of breath, persistent cough, fever, chest pain or injury. It also may be used to help diagnose and monitor treatment for a variety of lung conditions such as pneumonia, emphysema and cancer. Because chest X-ray is fast and easy, it is particularly useful in emergency diagnosis and treatment.\n",
    "\n",
    "Because of the difference in density between air, soft tissue and bone, the lungs appear much darker than their surroundings. Brighter regions in the lungs may indicate the presence of pathology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching Assistants\n",
    "- Ecem Lago: Ecem.Lago@radboudumc.nl\n",
    "- Mart van Rijthoven: Mart.vanrijthoven@radboudumc.nl\n",
    "\n",
    "Please submit your notebook via grand-challenge.org.\n",
    "Submit a notebook **WITH ALL CELLS EXECUTED!!!**\n",
    "\n",
    "* Groups: You should work in pairs or alone\n",
    "* Deadline for this assignment: \n",
    " * Thursday (March 14th) until midnight\n",
    " * 5 points (maximum grade = 100 points) penalization per day after deadline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks for this assignment \n",
    "\n",
    "The goal of this assignment is to get familiar with the YOLO architecture, loss function and training procedure, as well as the type of output it produces and how to transform it in actual predictions of bounding boxes. Additionally, you will get familiar with the problem of detecting lungs in chest X-ray, and the used architectures could be re-trained in the future to detect more anatomical structures in chest X-ray images.\n",
    "In order to get started with the YOLO architecture, we provide you with a YOLO network pre-trained to detect only the **right lung**, which you will first run on some images and then re-train to detect both left and right lungs.\n",
    "\n",
    "The three main tasks of this assignment are:\n",
    "\n",
    "* Task 1: Decode the network output to visualize the predicted bounding boxes for the right lung **(30 points)**\n",
    "* Task 2: Retrain the network on both lungs **(20 points)**\n",
    "* Task 3: Design a simplified YOLO architecture and train it from scratch **(50 points)**\n",
    "\n",
    "You may have noticed already that, differently from previous assignments, most of the points for your grade are at the end of the notebook. This is because the first two tasks heavily rely on code that we provide, and little changes have to be made there to get them to working. However, in the last part, a new architecture will have to be designed and re-trained, and in the notebook you will be building up knowledge and hands-on experience to understand how to design and train this network. We will propose to simplify the original YOLO architecture there (which was designed for object detection in natural images) and make a smaller one, which hopefully you can train in a smaller amount of time. More details about these tasks are provided later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data used in this assignment belong to the **CHESTXRAY14** dataset, which is publicly available and can be found at this link:https://nihcc.app.box.com/v/ChestXray-NIHCC. The dataset, released by the NIH, contains 112,120 frontal-view X-ray images of 30,805 unique patients, annotated with up to 14 different thoracic pathology labels using NLP methods on radiology reports.\n",
    "\n",
    "For this assignment, we have selected 13,331 chest X-ray images from CHESTXRAY14 and generated bounding boxes containing the left and the right lungs. The coordinates of bounding boxes are extracted from a previously obtained segmentation of the lungs, available in our research group (lung segmentation is not available in the CHESTXARY14 dataset), as depicted in the following example:\n",
    "\n",
    "<table width=\"100%\" border=\"0\">\n",
    "  <tr>\n",
    "  <td style=\"text-align:center\">chest x-ray image</td>\n",
    "  <td style=\"text-align:center\">lung segmentation</td>\n",
    "  <td style=\"text-align:center\">bounding box</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "  <td><img src=\"figures/00000007_000.PNG\" alt=\"\" align=\"center\" /></td>\n",
    "  <td><img src=\"figures/00000007_000_segmentation.PNG\" alt=\"\" align=\"center\" /></td>\n",
    "  <td><img src=\"figures/00000007_000_overlay.PNG\" alt=\"\" align=\"center\"/></td>\n",
    "\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Bounding box data come in ```xml``` format, which is the format read by the YOLO training script that we will use in this assignment (more details about this in next cells).\n",
    "An example of content of ```xml``` file for bounding boxes of right and left lung for the image ```00000004_000``` is the following:\n",
    "\n",
    "```xml\n",
    "<annotation verified=\"no\">\n",
    "  <folder>Lungs</folder>\n",
    "  <filename>00000004_000</filename>\n",
    "  <source>\n",
    "    <database>Unknown</database>\n",
    "  </source>\n",
    "  <size>\n",
    "    <width>512</width>\n",
    "    <height>512</height>\n",
    "    <depth>1</depth>\n",
    "  </size>\n",
    "  <segmented>0</segmented>\n",
    "  <object>\n",
    "    <name>RL</name>\n",
    "    <pose>Unspecified</pose>\n",
    "    <truncated>0</truncated>\n",
    "    <difficult>0</difficult>\n",
    "    <bndbox>\n",
    "      <xmin>64</xmin>\n",
    "      <ymin>62</ymin>\n",
    "      <xmax>256</xmax>\n",
    "      <ymax>490</ymax>\n",
    "    </bndbox>\n",
    "  </object>\n",
    "  <object>\n",
    "    <name>LL</name>\n",
    "    <pose>Unspecified</pose>\n",
    "    <truncated>0</truncated>\n",
    "    <difficult>0</difficult>\n",
    "    <bndbox>\n",
    "      <xmin>298</xmin>\n",
    "      <ymin>78</ymin>\n",
    "      <xmax>450</xmax>\n",
    "      <ymax>502</ymax>\n",
    "    </bndbox>\n",
    "  </object>\n",
    "</annotation>\n",
    "```\n",
    "\n",
    "Among all the parameters that you can find there, the most important ones are the following:\n",
    "\n",
    "* ```(width, height)```: image size\n",
    "* ```(RL, LL)```: labels for Right Lung and Left Lung\n",
    "* ```(xmin, xmax, ymin, ymax)```: coordinates of the top-left and bottom-right corners of the bounding box\n",
    "\n",
    "It is important that you understand these parameters now, because you will be using them in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the data and the weights of the pre-trained network needed for this assignment. If you are working on Cartesius, the data is already available in the project space. If you are working on your computer, you will have to download data and weights.\n",
    "\n",
    "First, let's import the libraries needed for this assignment.\n",
    "The code used in this notebook is developed based on this github repository: https://github.com/experiencor/basic-yolo-keras. For this reason, we will use the same libraries used in that repository, for compatibility reasons. Therefore, you will see that we will be using the ```opencv``` library to manipulate images, which we didn't do in previous assignments. Although in the lecture this week we have seen the basic ideas of YOLO, in this assignment we will use code of YOLOv2. Do not worry about that, the main ideas of the method are the same, and the only differences that you will notice in this assignment, compared to the original YOLO, are:\n",
    "* presence of skip connections in the architecture (like in ResNet)\n",
    "* use of an exponential function in the prediction of width and height of bounding boxes\n",
    "* normalization of width and height of anchor boxes based on grid cells size instead of image size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow as base library for neural networks\n",
    "import tensorflow as tf\n",
    "\n",
    "# keras as a layer on top of tensorflow\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "\n",
    "# h5py is needed to store and load Keras models\n",
    "import h5py\n",
    " \n",
    "# matplotlib is needed to plot bounding boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "%matplotlib inline\n",
    "\n",
    "import imgaug as ia # library for image augmentation, used in the repo code\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import os, cv2, time, random\n",
    "import xml.etree.ElementTree as ET # needed to read bounding boxes in xml format\n",
    "import ntpath\n",
    "\n",
    "# notebook-specific libraries, provided by the repo code\n",
    "from preprocessing import parse_annotation, BatchGenerator, normalize\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local machine\n",
    "\n",
    "\n",
    "If working on your local machine, use the following cells to download the training, validation and test datasets and set your working directory.\n",
    "\n",
    "If working on Cartesius proceed to the [next section](#cartesius)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download **training set** and reference standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "link = 'https://surfdrive.surf.nl/files/index.php/s/kwSOdtP4t3dh5vD/download'\n",
    "file_name = \"train_lung_detection.zip\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in tqdm_notebook(response.iter_content(chunk_size=4096), desc='Downloading data'):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "with zipfile.ZipFile(file_name,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "os.remove('./train_lung_detection.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download **test set** images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "link = 'https://surfdrive.surf.nl/files/index.php/s/ZImBQSvSCpqF0i9/download'\n",
    "file_name = \"test_images_lung_detection.zip\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in tqdm_notebook(response.iter_content(chunk_size=4096), desc='Downloading data'):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "with zipfile.ZipFile(file_name,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "os.remove('./test_images_lung_detection.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download parameters of pre-trained network for **right lung detection**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "link = 'https://surfdrive.surf.nl/files/index.php/s/LPzyrS0Tb9NgnTs/download'\n",
    "file_name = \"weights_right_lung.zip\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in tqdm_notebook(response.iter_content(chunk_size=4096), desc='Downloading data'):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "with zipfile.ZipFile(file_name,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "os.remove('./weights_right_lung.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download parameters of pre-trained network for **right and left lung detection**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "link = 'https://surfdrive.surf.nl/files/index.php/s/0eK0ob45rmeaOyf/download'\n",
    "file_name = \"weights_both_lungs.zip\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in tqdm_notebook(response.iter_content(chunk_size=4096), desc='Downloading data'):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "with zipfile.ZipFile(file_name,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "os.remove('./weights_both_lungs.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download parameters of pre-trained network for **Pascal VOC** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "link = 'https://surfdrive.surf.nl/files/index.php/s/HGmdukdYpnyt2NV/download'\n",
    "file_name = \"pretrained_yolo_weights.zip\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in tqdm_notebook(response.iter_content(chunk_size=4096), desc='Downloading data'):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "with zipfile.ZipFile(file_name,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "os.remove('./pretrained_yolo_weights.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workdir = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='cartesius'></a>\n",
    "### Cartesius\n",
    "\n",
    "For working on Cartesius correctly set your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workdir = '/projects/0/ismi2018/CHESTXRAY14/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you will see that the structure of ```workdir``` is the following:\n",
    "\n",
    "```python\n",
    "train                        # directory of training data\n",
    "  images                     # training images\n",
    "  xml                        # bounding boxes of right and left lungs \n",
    "  xml_right                  # bounding boxes of only right lung\n",
    "test_images                  # test images\n",
    "weights_right_lung.h5        # weights of pre-trained YOLO (only right lung detection)\n",
    "pretrained_yolo_weights.h5   # weights of pre-trained YOLO (VOC dataset)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Get to know the data and the YOLO network (max 30 points)\n",
    "\n",
    "In this section, you will:\n",
    "* visualize chest X-ray images from the training set\n",
    "* read bounding box data from the reference standard of the right lung\n",
    "* plot the bounding box\n",
    "\n",
    "After that, you will:\n",
    "* run the YOLO network, pre-trained to detect right lungs, on test images\n",
    "* decode the output tensor in order to extract bounding box information\n",
    "* visualize the predicted bounding box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load chest x-ray from training set\n",
    "To get to know the data you will be working with, first load an example of a chest x-ray image from the training set and visualize it below.\n",
    "\n",
    "**Note:** you can run the cells below multiple times to see the variability in the data! As you will notice, all images have a size of 512x512 px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define directory of training images\n",
    "train_dir_images = os.path.join(workdir, 'train', 'images')\n",
    "\n",
    "# pick random training image\n",
    "case = os.path.join(train_dir_images, random.choice(os.listdir(train_dir_images)))\n",
    "case_filename = os.path.splitext(ntpath.basename(case))[0] + '.xml'\n",
    "\n",
    "# open image with opencv and visualize it\n",
    "image = cv2.imread(case)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What kind of variability do you see in training examples and what can cause it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Your answer here --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images in the training data are accompanied by annotations of bounding boxes that surround the right lung. Each annotation consists of a single ```xml``` file with coordinates that define the edges of the boxes (```xmin, xmax, ymin, ymax```).\n",
    "\n",
    "Below you will find code that defines the class ```BoundBox``` that stores information of bounding boxes, but in a slightly different format, using ```(x, y)``` as the coordinates of the center of the bounding box, and ```(h, w)``` as its height and width. It also implements additional variables related to classes, scores and labels, which will become clear later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BoundingBox:\n",
    "    def __init__(self, x, y, w, h, c = None, classes = None):\n",
    "        self.x     = x\n",
    "        self.y     = y\n",
    "        self.w     = w\n",
    "        self.h     = h\n",
    "        \n",
    "        self.c     = c\n",
    "        self.classes = classes\n",
    "\n",
    "        self.label = -1\n",
    "        self.score = -1\n",
    "\n",
    "    def get_label(self):\n",
    "        if self.label == -1:\n",
    "            self.label = np.argmax(self.classes)        \n",
    "        return self.label\n",
    "    \n",
    "    def get_score(self):\n",
    "        if self.score == -1:\n",
    "            self.score = self.classes[self.get_label()]\n",
    "        return self.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a function to read annotation in ```xml``` format and convert them into parameters to pass to the ```BoundingBox``` class. Most of the code of this function is provided, but you will have to implement the last part of it (replace the ```None``` values).\n",
    "\n",
    "**NOTE**: as for many variables in the YOLO framework, ```(x, y, h, w)``` used in the ```BoundingBox``` class are normalized by the image height and width, meaning that they have values in the range ```[0.0, 1.0]```. Take this into account in your implementation!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xml2bbox(xml_file):\n",
    "    \n",
    "    # parse structure of XML file\n",
    "    tree = ET.parse(xml_file)\n",
    "\n",
    "    for elem in tree.iter():\n",
    "        if 'width' in elem.tag:\n",
    "            width = int(elem.text) # image width\n",
    "        if 'height' in elem.tag:\n",
    "            height = int(elem.text) # image height\n",
    "\n",
    "        for attr in list(elem):\n",
    "\n",
    "            if 'bndbox' in attr.tag:\n",
    "                for dim in list(attr):\n",
    "                    if 'xmin' in dim.tag:\n",
    "                        xmin = int(dim.text) # xmin bounding box\n",
    "                    if 'ymin' in dim.tag:\n",
    "                        ymin = int(dim.text) # ymin bounding box\n",
    "                    if 'xmax' in dim.tag:\n",
    "                        xmax = int(dim.text) # xmax bounding box\n",
    "                    if 'ymax' in dim.tag:\n",
    "                        ymax = int(dim.text) # ymax bounding box\n",
    "    \n",
    "    ### Replace None with your code ###\n",
    "    x = None\n",
    "    y = None\n",
    "    w = None\n",
    "    h = None\n",
    "    \n",
    "    return BoundingBox(x,y,w,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the ```BoundingBox``` class and the ```xml2bbox``` function, we can get the bounding box corresponding to the XML file annotation for the chosen training image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anno = os.path.join(workdir, 'train', 'xml_right', case_filename)\n",
    "bbox = xml2bbox(anno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is also convenient to define a function that allows us to plot bounding boxes using ```matplotlib```. We will use the ```Rectangle``` class for that. The following function reads a list of ```BoundingBox``` objects and returns a list of corresponding ```Rectangle``` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_matplotlib_boxes(boxes, img_shape):\n",
    "    plt_boxes = []\n",
    "    for box in boxes:\n",
    "        xmin  = int((box.x - box.w/2) * img_shape[1])\n",
    "        xmax  = int((box.x + box.w/2) * img_shape[1])\n",
    "        ymin  = int((box.y - box.h/2) * img_shape[0])\n",
    "        ymax  = int((box.y + box.h/2) * img_shape[0])        \n",
    "        plt_boxes.append(patches.Rectangle((xmin, ymin), xmax-xmin, ymax-ymin, fill=False, color='#00FF00', linewidth='2'))\n",
    "    return plt_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the code written so far to read, process and visualize the bounding box of the right lung together with the chest X-ray image visualized above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get bounding boxes in matplotlib format\n",
    "plt_boxes = get_matplotlib_boxes([bbox],image.shape)\n",
    "\n",
    "# visualize image and bounding box\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "plt.imshow(image.squeeze(), cmap='gray')\n",
    "for plt_box in plt_boxes:\n",
    "    ax.add_patch(plt_box)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Why do we talk about right lung when the bounding box is actually on the left of the image?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Your answer here --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained YOLO and run it on the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained a YOLO network to detect right lung in chest x-ray images already. The weights of the pre-trained network are available in the ```workdir```. We will use this pre-trained network now to get familiar with the network architecture.\n",
    "\n",
    "#### YOLO parameters\n",
    "As we learned in the lecture this week, the YOLO approach has several parameters to configure in order to learn to detect objects. All parameters are presented and explained in the YOLO and YOLO9000 (YOLOv2) papers. In particular, we know that YOLO divides the input image into a grid of cells, whose size have to be defined by the user.\n",
    "\n",
    "The list of parameters and their short description is the following:\n",
    "\n",
    "| Variable name| Description |\n",
    "|:--------------|:----------------------------------------------------------------------------------------------|\n",
    "| ```LABELS``` | List of strings about classes we want to learn (```RL``` = right lung, ```LL``` = left lung) |\n",
    "| ```IMAGE_H, IMAGE_W``` | Height and width of the input image |\n",
    "|```GRID_H,  GRID_W```| Number of grid cells in each dimension of the image. Grid cell size is ```(IMAGE_H/GRID_H, IMAGE_W/GRID_W)```|\n",
    "| ```BOX``` | Number of boxes that a single grid cell can predict (which is the number of anchors) |\n",
    "| ```CLASS``` | Number of classes |\n",
    "| ```CLASS_WEIGHTS``` | Array to define the importance of each class |\n",
    "| ```OBJ_THRESHOLD``` | Threshold used in final object detection |\n",
    "| ```NMS_THRESHOLD``` | Threshold used in non maximal suppression |\n",
    "| ```ANCHORS``` | List of coefficients of grid cell size (to be read two by two) that define the used anchors. This means that (```ANCHORS[0], ANCHORS[1]```) are the width and the height of the first anchor, (```ANCHORS[2], ANCHORS[3]```) are the widht and the height of the second anchor, etc. Differently from what presented in the original YOLO paper, and said in the lecture, anchors here are normalized by grid cell size, and not by image size, so we can have ```w >= 1.``` and ```h >= 1.```.|\n",
    "| ```NO_OBJECT_SCALE``` | Part of the loss function|\n",
    "| ```OBJECT_SCALE``` | Part of the loss function|\n",
    "| ```COORD_SCALE``` | Part of the loss function|\n",
    "| ```CLASS_SCALE``` | Part of the loss function|\n",
    "| ```BATCH_SIZE``` | Mini-batch size|\n",
    "| ```WARM_UP_BATCHES``` | Number of mini-batches used at the beginning of training to do warm-up (a different loss is applied) |\n",
    "| ```TRUE_BOX_BUFFER``` | Used to create list of ground truth boxes, which is needed for Keras implementation|\n",
    "\n",
    "All these parameters are set in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the parameters for the detection of the right lung\n",
    "\n",
    "LABELS = ['RL']\n",
    "\n",
    "IMAGE_H, IMAGE_W = 512, 512\n",
    "GRID_H,  GRID_W  = 16 , 16\n",
    "BOX              = 5\n",
    "CLASS            = len(LABELS)\n",
    "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
    "OBJ_THRESHOLD    = 0.3\n",
    "NMS_THRESHOLD    = 0.3\n",
    "ANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
    "\n",
    "NO_OBJECT_SCALE  = 1.0 # lambda noobj\n",
    "OBJECT_SCALE     = 5.0 # lambda obj\n",
    "COORD_SCALE      = 1.0 # don't touch this\n",
    "CLASS_SCALE      = 1.0 # don't touch this\n",
    "\n",
    "BATCH_SIZE       = 16\n",
    "WARM_UP_BATCHES  = 100\n",
    "TRUE_BOX_BUFFER  = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The YOLO network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the architecture of the YOLO network, as it comes with the code in the repo, which is very close to the one in the YOLO paper, and includes some features present in the YOLO9000 paper, as for example some kind of skip connections (like in ResNet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def space_to_depth_x2(x):\n",
    "    return tf.space_to_depth(x, block_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
    "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def YOLO_network(input_img,true_bxs,CLASS):\n",
    "\n",
    "    # Layer 1\n",
    "    x = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_img)\n",
    "    x = BatchNormalization(name='norm_1')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Layer 2\n",
    "    x = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_2')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Layer 3\n",
    "    x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_3')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 4\n",
    "    x = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_4')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 5\n",
    "    x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_5')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Layer 6\n",
    "    x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_6')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 7\n",
    "    x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_7')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 8\n",
    "    x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False, input_shape=(512,512,3))(x)\n",
    "    x = BatchNormalization(name='norm_8')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Layer 9\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_9')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 10\n",
    "    x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_10')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 11\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_11')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 12\n",
    "    x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_12')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 13\n",
    "    x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_13')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    skip_connection = x\n",
    "\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "    # Layer 14\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_14')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 15\n",
    "    x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_15')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 16\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_16')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 17\n",
    "    x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_17')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 18\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_18')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 19\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_19')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 20\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_20')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 21\n",
    "    skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
    "    skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
    "    skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
    "    skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
    "\n",
    "    x = concatenate([skip_connection, x])\n",
    "\n",
    "    # Layer 22\n",
    "    x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
    "    x = BatchNormalization(name='norm_22')(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "    # Layer 23\n",
    "    x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
    "    output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n",
    "\n",
    "    # small hack to allow true_boxes to be registered when Keras build the model \n",
    "    # for more information: https://github.com/fchollet/keras/issues/2790\n",
    "    output = Lambda(lambda args: args[0])([output, true_bxs])\n",
    "\n",
    "    model = Model([input_img, true_bxs], output)\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make an instance of the YOLO network by calling the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = YOLO_network(input_image, true_boxes, CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained weights of network for right lung detection\n",
    "\n",
    "Executing the cell below will load the weights of a model that has been trained on the right lung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wt_path = os.path.join(workdir, 'weights_right_lung.h5')\n",
    "model.load_weights(wt_path)\n",
    "print(\"Weights loaded from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to process one image from the test set using YOLO and inspect the format of the produced output. Since images were normalized to have an intensity between 0.0 and 1.0 during training, we will apply the same normalization strategy at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define directory of test images\n",
    "test_dir = os.path.join(workdir,'test_images')\n",
    "\n",
    "# pick a random test file\n",
    "test_img = os.path.join(test_dir, random.choice(os.listdir(test_dir)))\n",
    "\n",
    "# process image\n",
    "t_start = time.time()\n",
    "print(\"Processing\", test_img)\n",
    "image = cv2.imread(test_img)\n",
    "input_image = image / 255. # rescale intensity to [0, 1]\n",
    "input_image = input_image[:,:,::-1]\n",
    "img_shape = image.shape\n",
    "input_image = np.expand_dims(input_image, 0) \n",
    "\n",
    "# define variable needed to process input image\n",
    "dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
    "\n",
    "# get output from network\n",
    "netout = model.predict([input_image, dummy_array])\n",
    "\n",
    "print('processing one chest x-ray took {} seconds'.format(time.time() - t_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the ```shape``` of the tensor ```netout``` and try to understand what kind of information it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(netout.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you didn't change the default parameters of YOLO, you will see something like:\n",
    "\n",
    "```(1, 16, 16, 5, 6)```\n",
    "\n",
    "These values represent:\n",
    "\n",
    "```(mini_batch_size, grid_h, grid_w, n_bbox, ???)```\n",
    "\n",
    "In order to understand the meaning of the 5th dimension (which we indicate as ???), we ask you to do two things:\n",
    "\n",
    "1. read and try to understand the content of the following function ```decode_netout()```\n",
    "2. use the content of the lecture of this week, about object detection and YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_netout(netout, obj_threshold, nms_threshold, anchors, nb_class):\n",
    "    \"\"\"\n",
    "        Decode output tensor of YOLO network and return list of BoundingBox objects.\n",
    "    \"\"\"\n",
    "    grid_h, grid_w, nb_box = netout.shape[:3]\n",
    "\n",
    "    boxes = []\n",
    "    \n",
    "    # decode the output by the network\n",
    "    netout[..., 4]  = utils.sigmoid(netout[..., 4])\n",
    "    netout[..., 5:] = netout[..., 4][..., np.newaxis] * utils.softmax(netout[..., 5:])\n",
    "    netout[..., 5:] *= netout[..., 5:] > obj_threshold\n",
    "    \n",
    "    for row in range(grid_h):\n",
    "        for col in range(grid_w):\n",
    "            for b in range(nb_box):\n",
    "                # from 4th element onwards are confidence and class classes\n",
    "                classes = netout[row,col,b,5:]\n",
    "                \n",
    "                if classes.any():\n",
    "                    # first 4 elements are x, y, w, and h\n",
    "                    x, y, w, h = netout[row,col,b,:4]\n",
    "\n",
    "                    x = (col + utils.sigmoid(x)) / grid_w # center position, unit: image width\n",
    "                    y = (row + utils.sigmoid(y)) / grid_h # center position, unit: image height\n",
    "                    w = anchors[2 * b + 0] * np.exp(w) / grid_w # unit: image width\n",
    "                    h = anchors[2 * b + 1] * np.exp(h) / grid_h # unit: image height\n",
    "                    confidence = netout[row,col,b,4]\n",
    "                    \n",
    "                    box = BoundingBox(x, y, w, h, confidence, classes)\n",
    "                    \n",
    "                    boxes.append(box)\n",
    "\n",
    "    # suppress non-maximal boxes\n",
    "    for c in range(nb_class):\n",
    "        sorted_indices = list(reversed(np.argsort([box.classes[c] for box in boxes])))\n",
    "\n",
    "        for i in range(len(sorted_indices)):\n",
    "            index_i = sorted_indices[i]\n",
    "            \n",
    "            if boxes[index_i].classes[c] == 0: \n",
    "                continue\n",
    "            else:\n",
    "                for j in range(i+1, len(sorted_indices)):\n",
    "                    index_j = sorted_indices[j]\n",
    "                    \n",
    "                    if utils.bbox_iou(boxes[index_i], boxes[index_j]) >= nms_threshold:\n",
    "                        boxes[index_j].classes[c] = 0\n",
    "                        \n",
    "    # remove the boxes which are less likely than a obj_threshold\n",
    "    boxes = [box for box in boxes if box.get_score() > obj_threshold]\n",
    "    \n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What is the meaning of the 6 values in the 5th dimension of ```netout```?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Your answer here ---\n",
    "\n",
    "* ```value #1```:\n",
    "\n",
    "* ```value #2```:\n",
    "\n",
    "* ```value #3```:\n",
    "\n",
    "* ```value #4```:\n",
    "\n",
    "* ```value #5```:\n",
    "\n",
    "* ```value #6```:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can put all these things together and make a function that we can use to get predicted bounding boxes for test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_bounding_box(img, model, obj_threshold, nms_threshold, anchors, nb_class):\n",
    "    \"\"\"\n",
    "        Predict bounding boxes for a given image.\n",
    "    \"\"\"    \n",
    "    image = cv2.imread(img)\n",
    "    input_image = image / 255. # rescale intensity to [0, 1]\n",
    "    input_image = input_image[:,:,::-1]\n",
    "    img_shape = image.shape\n",
    "    input_image = np.expand_dims(input_image, 0) \n",
    "\n",
    "    # define variable needed to process input image\n",
    "    dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
    "\n",
    "    # get output from network\n",
    "    netout = model.predict([input_image, dummy_array])\n",
    "    \n",
    "    return decode_netout(netout[0], obj_threshold, nms_threshold, anchors, nb_class)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function we just defined to process our test image and visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a threshold to apply to predictions\n",
    "obj_threshold=0.2\n",
    "\n",
    "boxes = predict_bounding_box(test_img, model, obj_threshold, NMS_THRESHOLD, ANCHORS, CLASS)\n",
    "\n",
    "# get matplotlib bbox objects\n",
    "plt_boxes = get_matplotlib_boxes(boxes, img_shape)\n",
    "\n",
    "# visualize result\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "plt.imshow(cv2.imread(test_img), cmap='gray')\n",
    "for plt_box in plt_boxes:\n",
    "    ax.add_patch(plt_box)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: For this first test, we used a threshold ```obj_threshold=0.2```, which results in a meaningful bounding box for the right lung. Try to see what happens if you pick a much lower threshold, for example ```obj_threshold=0.0```, or ```obj_threshold=0.001``` and explain what you see**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Your answer here --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it would be good to check the performance of YOLO on the entire test set. Since the test set is far too large to process in this manner, if you think that the output of the cell above seems to be correct, run the cell below to process the first 50 images in the test dataset and visualize the output. Change ```obj_threshold``` to allow detection of bounding boxes with higher/lower confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_dir = './outputs'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "test_dir = os.path.join(workdir,'test_images')\n",
    "\n",
    "file_list = os.listdir(test_dir)\n",
    "\n",
    "obj_threshold=0.2\n",
    "\n",
    "for t in range(50):\n",
    "\n",
    "    test_img = os.path.join(test_dir,file_list[t])\n",
    "    \n",
    "    print('processing {}'.format(file_list[t]))\n",
    "\n",
    "    boxes = predict_bounding_box(test_img, model, obj_threshold, NMS_THRESHOLD, ANCHORS, CLASS)\n",
    "\n",
    "    # get matplotlib bbox objects\n",
    "    #\n",
    "    plt_boxes = get_matplotlib_boxes(boxes, img_shape)\n",
    "\n",
    "    # visualize results\n",
    "    #\n",
    "    fig1 = plt.figure(figsize=(10, 10))\n",
    "    ax1 = fig1.add_subplot(111, aspect='equal')\n",
    "    plt.imshow(cv2.imread(test_img).squeeze(), cmap='gray')\n",
    "    for plt_box in plt_boxes:\n",
    "        ax1.add_patch(plt_box)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2. Re-train YOLO to detect two lungs!\n",
    "Now that we have familiarized with the data, the network architecture and the output it produces, we can start re-training YOLO to detect, this time, both lungs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize BatchGenerator for training data\n",
    "Before we start training our network, we intialize a batch generator to sample images during training, both from the training set and from the validation set, similar to what was done in the previous assignments. In this case, we will use the class ```BatchGenerator``` defined in the Github repo. This class requires two input objects:\n",
    "\n",
    "* ```all_imgs```: list of images\n",
    "* ```generator_config```: dictionary of configuration parameters\n",
    "\n",
    "We define the ```generator_config``` first, in the next cell (we will reuse all the variables already used before, but with a \"2\" at the end, when needed, indicating that this time it is for 2 lungs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define configuration parameters for batch generator\n",
    "\n",
    "LABELS2 = ['RL','LL'] # RL = right lung, LL = left lung\n",
    "\n",
    "CLASS2            = len(LABELS2)\n",
    "CLASS_WEIGHTS2    = np.ones(CLASS2, dtype='float32')\n",
    "\n",
    "generator_config = {\n",
    "    'IMAGE_H'         : IMAGE_H, \n",
    "    'IMAGE_W'         : IMAGE_W,\n",
    "    'GRID_H'          : GRID_H,  \n",
    "    'GRID_W'          : GRID_W,\n",
    "    'BOX'             : BOX,\n",
    "    'LABELS'          : LABELS2,\n",
    "    'CLASS'           : len(LABELS2),\n",
    "    'ANCHORS'         : ANCHORS,\n",
    "    'BATCH_SIZE'      : BATCH_SIZE,\n",
    "    'TRUE_BOX_BUFFER' : 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_image_2 = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
    "true_boxes_2  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
    "\n",
    "model_2 = YOLO_network(input_image_2, true_boxes_2, CLASS2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the list of images ```all_imgs``` by reading all training images and corresponding annotations. This may take a while, because we have 10,000 training images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract list of images and corresponding annotations\n",
    "\n",
    "image_path = os.path.join(workdir, 'train', 'images/')\n",
    "annot_path = os.path.join(workdir, 'train', 'xml/')\n",
    "\n",
    "all_imgs, seen_labels = parse_annotation(annot_path, image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ```BatchGenerator``` for training and validation sets. In order to do that, we have to define the percentage of data we want to use for training and for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define percentage of data to use for training\n",
    "training_data_percentage = None # set a number between 0 and 1\n",
    "\n",
    "# define number of training images\n",
    "train_valid_split = int(training_data_percentage * len(all_imgs))\n",
    "\n",
    "# initialize training and validation batch generators\n",
    "train_batch = BatchGenerator(all_imgs[:train_valid_split], generator_config, norm=utils.normalize)\n",
    "valid_batch = BatchGenerator(all_imgs[train_valid_split:], generator_config, norm=utils.normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "We have seen in the lecture that the YOLO network has a loss function that contains several elements. Here we report the formula of the loss function, and also its implementation as provided with the repo code. As you will see, it's a quite long and complex piece of code. We report the formula and the code here for completeness, but you don't have to worry too much about it now, just use it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{multline}\n",
    "\\lambda_\\textbf{coord}\n",
    "\\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "     L_{ij}^{\\text{obj}}\n",
    "            \\left[\n",
    "            \\left(\n",
    "                x_i - \\hat{x}_i\n",
    "            \\right)^2 +\n",
    "            \\left(\n",
    "                y_i - \\hat{y}_i\n",
    "            \\right)^2\n",
    "            \\right]\n",
    "\\\\\n",
    "+ \\lambda_\\textbf{coord} \n",
    "\\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "         L_{ij}^{\\text{obj}}\n",
    "         \\left[\n",
    "        \\left(\n",
    "            \\sqrt{w_i} - \\sqrt{\\hat{w}_i}\n",
    "        \\right)^2 +\n",
    "        \\left(\n",
    "            \\sqrt{h_i} - \\sqrt{\\hat{h}_i}\n",
    "        \\right)^2\n",
    "        \\right]\n",
    "\\\\\n",
    "+ \\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "        L_{ij}^{\\text{obj}}\n",
    "        \\left(\n",
    "            C_i - \\hat{C}_i\n",
    "        \\right)^2\n",
    "\\\\\n",
    "+ \\lambda_\\textrm{noobj}\n",
    "\\sum_{i = 0}^{S^2}\n",
    "    \\sum_{j = 0}^{B}\n",
    "    L_{ij}^{\\text{noobj}}\n",
    "        \\left(\n",
    "            C_i - \\hat{C}_i\n",
    "        \\right)^2\n",
    "\\\\\n",
    "+ \\sum_{i = 0}^{S^2}\n",
    "L_i^{\\text{obj}}\n",
    "    \\sum_{c \\in \\textrm{classes}}\n",
    "        \\left(\n",
    "            p_i(c) - \\hat{p}_i(c)\n",
    "        \\right)^2\n",
    "\\end{multline}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "        A custom loss is defined for YOLO, which is not implemented in Keras.\n",
    "    \"\"\"\n",
    "    \n",
    "    mask_shape = tf.shape(y_true)[:4]\n",
    "    \n",
    "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "\n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, BOX, 1])\n",
    "    \n",
    "    coord_mask = tf.zeros(mask_shape)\n",
    "    conf_mask  = tf.zeros(mask_shape)\n",
    "    class_mask = tf.zeros(mask_shape)\n",
    "    \n",
    "    seen = tf.Variable(0.)\n",
    "    total_recall = tf.Variable(0.)\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust prediction\n",
    "    \"\"\"\n",
    "    ### adjust x and y      \n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
    "    \n",
    "    ### adjust w and h\n",
    "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS, [1,1,1,BOX,2])\n",
    "    \n",
    "    ### adjust confidence\n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
    "    \n",
    "    ### adjust class probabilities\n",
    "    pred_box_class = y_pred[..., 5:]\n",
    "    \n",
    "    \"\"\"\n",
    "    Adjust ground truth\n",
    "    \"\"\"\n",
    "    ### adjust x and y\n",
    "    true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
    "    \n",
    "    ### adjust w and h\n",
    "    true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
    "    \n",
    "    ### adjust confidence\n",
    "    true_wh_half = true_box_wh / 2.\n",
    "    true_mins    = true_box_xy - true_wh_half\n",
    "    true_maxes   = true_box_xy + true_wh_half\n",
    "    \n",
    "    pred_wh_half = pred_box_wh / 2.\n",
    "    pred_mins    = pred_box_xy - pred_wh_half\n",
    "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
    "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "    \n",
    "    true_box_conf = iou_scores * y_true[..., 4]\n",
    "    \n",
    "    ### adjust class probabilities\n",
    "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "    \n",
    "    \"\"\"\n",
    "    Determine the masks\n",
    "    \"\"\"\n",
    "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
    "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * COORD_SCALE\n",
    "    \n",
    "    ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
    "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
    "    true_xy = true_boxes_2[..., 0:2]\n",
    "    true_wh = true_boxes_2[..., 2:4]\n",
    "    \n",
    "    true_wh_half = true_wh / 2.\n",
    "    true_mins    = true_xy - true_wh_half\n",
    "    true_maxes   = true_xy + true_wh_half\n",
    "    \n",
    "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
    "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
    "    \n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins    = pred_xy - pred_wh_half\n",
    "    pred_maxes   = pred_xy + pred_wh_half    \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
    "    conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
    "    \n",
    "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "    conf_mask = conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
    "    \n",
    "    ### class mask: simply the position of the ground truth boxes (the predictors)\n",
    "    class_mask = y_true[..., 4] * tf.gather(CLASS_WEIGHTS2, true_box_class) * CLASS_SCALE       \n",
    "    \n",
    "    \"\"\"\n",
    "    Warm-up training\n",
    "    \"\"\"\n",
    "    no_boxes_mask = tf.to_float(coord_mask < COORD_SCALE/2.)\n",
    "    seen = tf.assign_add(seen, 1.)\n",
    "    \n",
    "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES), \n",
    "                          lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
    "                                   true_box_wh + tf.ones_like(true_box_wh) * np.reshape(ANCHORS, [1,1,1,BOX,2]) * no_boxes_mask, \n",
    "                                   tf.ones_like(coord_mask)],\n",
    "                          lambda: [true_box_xy, \n",
    "                                   true_box_wh,\n",
    "                                   coord_mask])\n",
    "    \n",
    "    \"\"\"\n",
    "    Finalize the loss\n",
    "    \"\"\"\n",
    "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
    "    nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
    "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
    "    \n",
    "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
    "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
    "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
    "    \n",
    "    loss = loss_xy + loss_wh + loss_conf + loss_class\n",
    "    \n",
    "    nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
    "    nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
    "\n",
    "    \"\"\"\n",
    "    Debugging code\n",
    "    \"\"\"    \n",
    "    current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
    "    total_recall = tf.assign_add(total_recall, current_recall) \n",
    "\n",
    "    loss = tf.Print(loss, [tf.zeros((1))], message='Dummy Line \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
    "    loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to defining the loss function, we define some extra information that is useful during training, namely a criterion for early stopping, to avoid overfitting of the network, and another criterion for saving the weights of the best network during training, namely the one that minimized the validation loss. The weights of that network will be saved as ```weights_left_right_lung.h5```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', \n",
    "                           min_delta=0.001, \n",
    "                           patience=10, \n",
    "                           mode='min', \n",
    "                           verbose=1)\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights_left_right_lung.h5',\n",
    "                             monitor='val_loss', \n",
    "                             verbose=1, \n",
    "                             save_best_only=True, \n",
    "                             mode='min', \n",
    "                             period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load weights of network pre-trained on Pascal VOC data\n",
    "The YOLO network that we have inialized has random initial parameters. However, we know that YOLO was originally presented to process images in the Pascal VOC dataset, to tackle object detection in natural images with 20 different classes. You can imagine that a lot of information about structures that you can find in images, ranging from edges and corners to more complex structures like the ones you find in natural images, has been learned by the network already. For this reason, it is often a good idea to use a \"pre-trained\" network, by loading the parameters that have been learned from another dataset. In this case, we have the parameters of YOLO trained on Pascal VOC available, and we will load them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to file of Pascal VOC YOLO weights\n",
    "wt_path2 = os.path.join(os.getcwd(), 'pretrained_yolo_weights.h5')\n",
    "model_2.load_weights(wt_path2)\n",
    "print(\"Weights loaded from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "So far, we have:\n",
    "* made an instance of the YOLO network\n",
    "* loaded weights of YOLO pre-trained on Pascal VOC\n",
    "* defined a loss function\n",
    "* defined training set and validation set for detection of left and right lungs\n",
    "* defined batch generator for training and validation set\n",
    "\n",
    "Now we are ready to train our network!\n",
    "\n",
    "In the next cell, we suggest some values for the number of epochs and for the learning rate. Try to use these two values at first, which can be useful to (1) get quite stable loss values (we have seen that high learning rates often give NaN or very high loss values), and (2) realize how long it takes to train YOLO (using the current settings on cartesius should take ~ 15 minutes/epoch!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define number of epochs\n",
    "n_epoch = 2\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# define a folder where to store log files during training\n",
    "logwrite = './logs'\n",
    "if not os.path.exists(logwrite):\n",
    "    os.makedirs(logwrite)\n",
    "\n",
    "# Define Adam optimizer\n",
    "optimizer = Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
    "\n",
    "# compile YOLO model\n",
    "model_2.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "# do training!\n",
    "model_2.fit_generator(generator   = train_batch, \n",
    "                    steps_per_epoch  = len(train_batch), \n",
    "                    epochs           = n_epoch, \n",
    "                    verbose          = 1,\n",
    "                    validation_data  = valid_batch,\n",
    "                    validation_steps = len(valid_batch),\n",
    "                    callbacks        = [early_stop, checkpoint], \n",
    "                    max_queue_size   = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training for few epochs, we can apply the network to a randomly sampled image from the test set, and check whether the result makes sense. Adjust the value ```obj_threshold``` to get the best output from YOLO, out of all bounding boxes detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read from test set\n",
    "test_dir = os.path.join(workdir,'test_images')\n",
    "test_file_list = os.listdir(test_dir)\n",
    "img_index = np.random.randint(0, len(test_file_list))\n",
    "test_img = cv2.imread(os.path.join(test_dir, test_file_list[img_index]))\n",
    "\n",
    "# define a threshold to apply to predictions\n",
    "obj_threshold=0.01\n",
    "\n",
    "# predict bounding boxes using YOLO model2\n",
    "boxes = predict_bounding_box(os.path.join(test_dir, test_file_list[img_index]), model_2, obj_threshold, NMS_THRESHOLD, ANCHORS, CLASS2)\n",
    "print(test_file_list[img_index])\n",
    "# get matplotlib bbox objects\n",
    "plt_boxes = get_matplotlib_boxes(boxes, test_img.shape)\n",
    "\n",
    "# visualize result\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "plt.imshow(test_img, cmap='gray')\n",
    "for plt_box in plt_boxes:\n",
    "    ax.add_patch(plt_box)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, if you have trained YOLO for only few epochs, you have probably realized that the performance is not good. We did some experiments with the current architecture and we saw that after training for ~ 20 epochs, the performance is actually very good! So at this point you have two choices: (1) you can train the current architecture for ~ 5 hours, which may be a task for Cartesius to do overnight, or (2) see what the performance could be by loading the weights of YOLO trained to detect the two lungs, which we did for you already! You can load those weights by executing the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path to file of Pascal VOC YOLO weights\n",
    "wt_path2 = os.path.join(os.getcwd(), 'weights_both_lungs.h5')\n",
    "model_2.load_weights(wt_path2)\n",
    "print(\"Weights loaded from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can test images again and check if the predictions make sense. The difference between the network you trained yourself for few epochs and the one you just loaded is just the training time! In this case, just training longer improves the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read from test set\n",
    "test_dir = os.path.join(workdir,'test_images')\n",
    "test_file_list = os.listdir(test_dir)\n",
    "img_index = np.random.randint(0, len(test_file_list))\n",
    "test_img = cv2.imread(os.path.join(test_dir, test_file_list[img_index]))\n",
    "\n",
    "# define a threshold to apply to predictions\n",
    "obj_threshold=0.01\n",
    "\n",
    "# predict bounding boxes using YOLO model2\n",
    "boxes = predict_bounding_box(os.path.join(test_dir, test_file_list[img_index]), model_2, obj_threshold, NMS_THRESHOLD, ANCHORS, CLASS2)\n",
    "print(test_file_list[img_index])\n",
    "# get matplotlib bbox objects\n",
    "plt_boxes = get_matplotlib_boxes(boxes, test_img.shape)\n",
    "\n",
    "# visualize result\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "plt.imshow(test_img, cmap='gray')\n",
    "for plt_box in plt_boxes:\n",
    "    ax.add_patch(plt_box)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Make your own YOLO! (50 points)\n",
    "\n",
    "The main message of the steps of this assignment so far is that YOLO is a powerful architecture for object detection, and that it works very well for lungs detection in chest x-ray images, but training is somehow slow, and the architecture is quite deep.\n",
    "Moreover, several parameters that you have used so far were specifically tuned to address the object detection problem in the Pascal VOC dataset. Things like number of boxes, or anchor boxes, or grid cells may not be ideal for the problem at hand.\n",
    "Finally, you may wonder whether this quite deep architecture (23 layers) is really needed for the task of lung detection. Consider that this network was originally designed to detect 20 different classes in natural images, with objects at multiple scales, multiple shape, etc. In the field of chest x-ray, images are always the same size, and lungs can be expected in more or less the same position and comparable size/shape. Therefore, the number of bonding/anchor boxes that you can use can be tuned, as well as the shape of anchor boxes.\n",
    "\n",
    "The steps we propose for this task are the following:\n",
    "\n",
    "1. Think about the best choice for bounding boxes and anchor boxes, and define new values for the variables ```BOX``` and ```ANCHORS```.\n",
    "2. Think about a good choice for grid cells to use, and modify the variables ```GRID_H, GRID_W``` accordingly\n",
    "3. Define a simpler network architecture, meaning with fewer parameters (it could be shallower, or just with fewer filters per layer, or both), and train it to detect both lungs. You will have to do this with random initialization, because now pre-trained weights cannot be applied anymore!\n",
    "\n",
    "In case you have problems getting a good convergence, try to use your knowledge to make things work better (regularization, different optimizer, etc.)\n",
    "\n",
    "Do not modify previous cells, but re-use previous code by copying to the next cell (and add more cells if needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE. DO NOT MODIFY PREVIOUS CELLS! JUST COPY THE CODE THAT IS NEEDED TO INITIALIZE AND TRAIN THE NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit results to grand-challenge.org\n",
    "Now that you have trained your YOLO architecture, you can process the entire test set, save the predicted bounding boxes in a ```CSV``` file, and submit your results to grand-challenge.org. The results you will submit consist of the coordinates of top-left and bottom-right corners of bounding boxes for left and right lungs. The function in the next cell does the conversion of these coordinates from ```BoundingBox``` objects to a ```CVS``` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bbox2csv(image, filename, boxes, csv_file):\n",
    "    \n",
    "    csv_out = open(csv_file, 'a')\n",
    "    \n",
    "    line = filename.split('.')[0] #+ ',' + str(image.shape[1]) + ',' + str(image.shape[0])\n",
    "    \n",
    "    if boxes: # At least one bounding box is found.\n",
    "\n",
    "        maxL = -1\n",
    "        posL = -1\n",
    "        maxR = -1\n",
    "        posR = -1\n",
    "        for idx, box in enumerate(boxes):\n",
    "            if box.get_label()==0:\n",
    "                if box.get_score()>maxR:\n",
    "                    maxR = box.get_score()\n",
    "                    posR = idx\n",
    "            if box.get_label()==1:\n",
    "                if box.get_score()>maxL:\n",
    "                    maxL = box.get_score()\n",
    "                    posL = idx\n",
    "        \n",
    "        if posR>=0: # Bounding box for right lung.\n",
    "            box = boxes[posR]\n",
    "\n",
    "            xmin  = int((box.x - box.w/2) * img_shape[1])\n",
    "            xmax  = int((box.x + box.w/2) * img_shape[1])\n",
    "            ymin  = int((box.y - box.h/2) * img_shape[0])\n",
    "            ymax  = int((box.y + box.h/2) * img_shape[0])\n",
    "\n",
    "            line = line + ',' + str(xmin) + ',' + str(ymin) + ',' + str(xmax) + ',' + str(ymax)\n",
    "        else: # No bounding box for right lung.\n",
    "            line = line + ',' + str(0) + ',' + str(0) + ',' + str(0) + ',' + str(0)\n",
    "            \n",
    "        if posL>=0: # Bounding box for left lung.\n",
    "            box = boxes[posL]\n",
    "\n",
    "            xmin  = int((box.x - box.w/2) * img_shape[1])\n",
    "            xmax  = int((box.x + box.w/2) * img_shape[1])\n",
    "            ymin  = int((box.y - box.h/2) * img_shape[0])\n",
    "            ymax  = int((box.y + box.h/2) * img_shape[0])\n",
    "\n",
    "            line = line + ',' + str(xmin) + ',' + str(ymin) + ',' + str(xmax) + ',' + str(ymax)\n",
    "        else: # No bounding box for left lung.\n",
    "            line = line + ',' + str(0) + ',' + str(0) + ',' + str(0) + ',' + str(0)\n",
    "    else:\n",
    "        ## If no boxes detected == No lungs detected\n",
    "        line = line + ',' + str(0) + ',' + str(0) + ',' + str(0) + ',' + str(0)\n",
    "        line = line + ',' + str(0) + ',' + str(0) + ',' + str(0) + ',' + str(0) \n",
    "    csv_out.write(line + '\\n')\n",
    "    csv_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use this function to create the CSV file to submit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "out_dir = './outputs'\n",
    "\n",
    "test_dir = os.path.join(workdir,'test_images')\n",
    "\n",
    "file_list = os.listdir(test_dir)\n",
    "\n",
    "header = 'filename,RL_xmin,RL_ymin,RL_xmax,RL_ymax,LL_xmin,LL_ymin,LL_xmax,LL_ymax\\n'\n",
    "csv_file = os.path.join(outdir,'test_set.csv')\n",
    "\n",
    "with open(csv_file, 'w') as csv_out:\n",
    "    csv_out.write(header)\n",
    "\n",
    "for t in tqdm.tqdm(range(len(file_list))):\n",
    "\n",
    "    test_img = os.path.join(test_dir,file_list[t])\n",
    "\n",
    "    image = cv2.imread(test_img)\n",
    "    input_image = image / 255.\n",
    "    input_image = input_image[:,:,::-1]\n",
    "    img_shape = image.shape\n",
    "    input_image = np.expand_dims(input_image, 0) \n",
    "\n",
    "    dummy_array = np.zeros((1,1,1,1,TRUE_BOX_BUFFER,4))\n",
    "\n",
    "    netout = model_2.predict([input_image, dummy_array])\n",
    "\n",
    "    # decode the output of the network to obtain bounding box\n",
    "    #\n",
    "    boxes = decode_netout(netout[0], \n",
    "                          obj_threshold=0.5,\n",
    "                          nms_threshold=NMS_THRESHOLD,\n",
    "                          anchors=ANCHORS, \n",
    "                          nb_class=CLASS2)\n",
    "        \n",
    "    bbox2csv(image, file_list[t], boxes, csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit your results\n",
    "\n",
    "You can now download this csvfile with this link: [test_set.csv](test_set.csv).  \n",
    "Next, upload your result to the challenge website (https://ismi-chestxray.grand-challenge.org/) and see how well you performed compared to your fellow students! You can submit as often as you want, only the best result counts.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
