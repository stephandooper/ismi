{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vessel segmentation in retina fundus images\n",
    "<img src=\"images/21_training.png\" width=\"250\" height=\"250\" align=\"right\">\n",
    "\n",
    "In this assignment we will return to the taks of segmentation of vessels in retina fundus images from assignment 2. This time however, we will solve the problem using deep learning. More specifically, we will explore different **fully convolutional neural networks**.\n",
    "\n",
    "## Teaching assistants\n",
    "\n",
    "- Bart Liefers: Bart.Liefers@radboudumc.nl\n",
    "- Kevin Koschmieder: kevin.koschmieder@radboudumc.nl\n",
    "\n",
    "Please submit your notebook via grand-challenge.org (https://drive.grand-challenge.org/).\n",
    "Submit a notebook **WITH ALL CELLS EXECUTED!!!**\n",
    "\n",
    "* Groups: You should work in small groups (max 3 people) or alone\n",
    "* Deadline for this assignment: \n",
    " * Monday, March 4th until 23:59h\n",
    " * 5 points (maximum grade = 100 points) penalization per day after deadline\n",
    "* Submit your **fully executed** notebook to the grand-challenge.org platform\n",
    "* The file name of the notebook you submit must be NameSurname1_NameSurname2_NameSurname3.ipynb\n",
    "* The grades will be available before March 11th (tentative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Students\n",
    "Please fill in this cell with your name and e-mail address. This information will be used to grade your assignment.\n",
    "\n",
    "* Name student #1, email address: ...\n",
    "* Name student #2, email address: ...\n",
    "* Name student #3, email address: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "For this assignment we will use data from the publicly available DRIVE dataset (http://www.isi.uu.nl/Research/Databases/DRIVE/).\n",
    "The DRIVE dataset consists of 40 images, 20 used for training and 20 used for testing. Each case contains:\n",
    "* a fundus (RGB) image\n",
    "* a binary mask, which indicates the area of the image that has to be analyzed (removing black background)\n",
    "* manual annotations of retinal vessels, provided as a binary map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully convolutional neural networks\n",
    "In previous assignments we've seen how neural networks can be applied to classify images into different categories. In this assignment we will explore how the same techniques can be applied to a segmentation task. For segmentation we conceptually solve the same taks as for classification: we can extract a patch around every pixel, and then classify the patch and assign the output back to the pixel location. As you can imagine, many classification steps have to be done to segment a full image: we will be working with images of size 584x565 (329960 pixels), so that would mean we have to apply 329960 classification steps to segment the full image.\n",
    "\n",
    "However, when we move our patch by one pixel to classify the next pixel, a large proportion of the pixels of the new patch are identical to the previous patch, and we will be applying the same convolutions many times over. A trick to obtain a label for all pixels efficiently, is to define a network architecture that does not include a fully connected (or dense) part. As the network only contains convolutional filters, the spatial structure is kept intact. Therefore the whole network can be thought of as a single large convolutional filter that can be applied to the whole image at once. This speeds up the classification greatly and allows full image segmentation in milliseconds!\n",
    "\n",
    "During training the same trick can be applied: use a full image as input, and train on all pixel-labels synchronously. In this assignment however, we will initially train the network on patches extracted from random locations in the training data.\n",
    "There are several reasons why this could be useful. To name a few:\n",
    "* In some applications you don't have a label for all pixels, so patches are a necessity.\n",
    "* Less memory is needed during training which may speed up training time dramatically.\n",
    "* Per training iteration we can use patches from different source images, and apply augmentations on a patch-base, which increases diversity during training, leading to faster convergence and better results.\n",
    "* Patch sampling allows us to control a possible class imbalancing in an intuitive way.\n",
    "* By sampling more frequently from more difficult locations, we could control the difficulty of the training procedure carefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks \n",
    "The tasks you have to perform for this assignment are:\n",
    "1. Create a patch-extractor and a batch-creator to generate training data for the network **(20 pts)**\n",
    "2. Implement and train different fully convolutional neural networks **(50 pts)**\n",
    "3. Perform some experiments to improve performance of your model **(30 pts)**  \n",
    "There will be another additional **(10, 8, 6, 4, 2) pts** for the top 5 best scoring submissions to grand-challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Let's get started by importing libraries needed for this assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from scipy.ndimage.interpolation import rotate\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "import keras.callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to download the dataset to your local computer\n",
    "import requests\n",
    "from tqdm import tqdm_notebook \n",
    "import zipfile\n",
    "link = 'https://surfdrive.surf.nl/files/index.php/s/Kn4hCF4G919ijr3/download'\n",
    "file_name = \"DRIVE.zip\"\n",
    "with open(file_name, \"wb\") as f:\n",
    "        response = requests.get(link, stream=True)\n",
    "        total_length = response.headers.get('content-length')\n",
    "        if total_length is None: # no content length header\n",
    "            f.write(response.content)\n",
    "        else:\n",
    "            dl = 0\n",
    "            total_length = int(total_length)\n",
    "            for data in tqdm_notebook(response.iter_content(chunk_size=4096), desc='Downloading data'):\n",
    "                dl += len(data)\n",
    "                f.write(data)\n",
    "with zipfile.ZipFile(file_name,\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./\")\n",
    "os.remove(file_name)\n",
    "data_folder = 'DRIVE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a training and validation set\n",
    "We will load all training images into memory, and then divide them into two sets: one (called the **training set**) to optimize the weights of the network, and one (**validation set**) to monitor the performance of the network on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(path, ext=''):\n",
    "    return sorted([os.path.join(path, f) for f in os.listdir(path) if f.endswith(ext)])\n",
    "\n",
    "def load_img(path):\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "train_img_files = get_file_list(os.path.join(data_folder, 'training', 'images'), 'tif')\n",
    "train_msk_files = get_file_list(os.path.join(data_folder, 'training', 'mask'), 'gif')\n",
    "train_lbl_files = get_file_list(os.path.join(data_folder, 'training', '1st_manual'), 'gif')\n",
    "\n",
    "train_imgs = [load_img(f) for f in train_img_files]\n",
    "train_msks = [load_img(f) for f in train_msk_files]\n",
    "train_lbls = [load_img(f) for f in train_lbl_files]\n",
    "\n",
    "# we also load test image and masks, to be used later\n",
    "test_img_files = get_file_list(os.path.join(data_folder, 'test', 'images'), 'tif')\n",
    "test_msk_files = get_file_list(os.path.join(data_folder, 'test', 'mask'), 'gif')\n",
    "\n",
    "test_imgs = [load_img(f) for f in test_img_files]\n",
    "test_msks = [load_img(f) for f in test_msk_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define how many images will be used for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of validation images here:\n",
    "n_validation_imgs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a class ```DataSet``` that will be used to handle training and validation datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    \n",
    "    def __init__(self, imgs, msks, lbls=None):\n",
    "        self.imgs = imgs\n",
    "        self.msks = msks\n",
    "        self.lbls = lbls\n",
    "    \n",
    "    def show_image(self, i):\n",
    "        if self.lbls != None:\n",
    "            f, axes = plt.subplots(1, 3)\n",
    "            for ax, im, t in zip(axes, \n",
    "                                 (self.imgs[i], self.msks[i], self.lbls[i]), \n",
    "                                 ('RGB image', 'Mask','Manual annotation')):\n",
    "                ax.imshow(im, cmap='gray')\n",
    "                ax.set_title(t)\n",
    "        else:\n",
    "            f, axes = plt.subplots(1, 2)\n",
    "            for ax, im, t in zip(axes, \n",
    "                                 (self.imgs[i], self.msks[i]), \n",
    "                                 ('RGB image', 'Mask')):\n",
    "                ax.imshow(im, cmap='gray')\n",
    "                ax.set_title(t)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the first images as validation\n",
    "validation_data = DataSet(train_imgs[:n_validation_imgs], train_msks[:n_validation_imgs], train_lbls[:n_validation_imgs])\n",
    "\n",
    "# the rest as training\n",
    "train_data = DataSet(train_imgs[n_validation_imgs:], train_msks[n_validation_imgs:], train_lbls[n_validation_imgs:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect some of the loaded images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (20, 12)\n",
    "validation_data.show_image(0) # change this parameter to try a few images\n",
    "train_data.show_image(0) # change this parameter to try a few images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a patch extractor (15pts)\n",
    "Differently from what we've done in the past two weeks, in this assignment we are going to implement a ```PatchExtractor``` class, which extract random patches from a list of images.\n",
    "This means that you don't need to create a dataset beforehand and then use it to train you network, but you will just have a list of training images available, and patches will be extracted **on-the-fly** during training.\n",
    "This strategy allows to save time in the preparation of your *static* dataset, and allows the use of a *dynamic* generation of batches, where data augmentation can also be applied on-the-fly.\n",
    "Note that this approach allows to test different strategies of data augmentation without the need for making a new dataset from scratch all the time.\n",
    "\n",
    "For now, we will only implement one kind of data augmentation in the ```get_patch``` method: **horizontal flipping**. \n",
    "The event will occur at random, meaning that during patch extraction, some of the extracted patches will be randomly transformed. \n",
    "\n",
    "Later in the assignment you will also use the ```get_image``` method, which also performs rotation-augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the get_patch method below:\n",
    "\n",
    "class PatchExtractor:\n",
    "\n",
    "    def __init__(self, patch_size, max_rotation=30, flipping=True):\n",
    "        self.patch_size = patch_size \n",
    "        self.flipping = flipping\n",
    "        self.max_rotation = max_rotation\n",
    "        \n",
    "        \n",
    "    def get_patch(self, image, location):\n",
    "        ''' \n",
    "        image: a numpy array representing the input image\n",
    "        location: a tuple with an y and x coordinate\n",
    "        \n",
    "        return a patch from the image at `location`, representing the top left corner of the patch\n",
    "        if self.flipping = True, there is a 50% chance the patch is horizontally flipped   \n",
    "        we will not rotate it or perform other augmentations for now to speed up the training process\n",
    "        '''\n",
    "        y, x = location      \n",
    "        h, w = self.patch_size\n",
    "        \n",
    "        ### Your code starts here\n",
    "        \n",
    "        # - patch should be a numpy array of size <h, w>\n",
    "        # - the patch should be normalized (intensity values between 0-1)\n",
    "        patch = None\n",
    "\n",
    "        # - if self.flipping = True, there should be a 50% chance to apply a horizontal flip to the patch  \n",
    "        if self.flipping:\n",
    "            do_flipping = None\n",
    "            \n",
    "        # - if do_flipping == True, flip the patch horizontally\n",
    "        if do_flipping:\n",
    "            patch = None\n",
    "        \n",
    "        ### Your code ends here\n",
    "        \n",
    "        return patch\n",
    "    \n",
    "    def get_image(self, image, label):\n",
    "        ''' \n",
    "        image: a numpy array representing the input image\n",
    "        label: a numpy array representing the labels corresponding to input image\n",
    "        return a rotated and flipped version of the image and corresponding label\n",
    "        '''\n",
    "        angle = 2 * (random.random() - 0.5) * self.max_rotation\n",
    "        image_out = rotate(image, angle, reshape=False, order=0) / 255\n",
    "        label_out = rotate(label, angle, reshape=False, order=0) > 0\n",
    "\n",
    "        flip = self.flipping and random.random() > 0.5\n",
    "        if flip:\n",
    "            image_out = np.fliplr(image_out)\n",
    "            label_out = np.fliplr(label_out)\n",
    "            \n",
    "        return image_out, label_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our patch extractor! By repeatedly executing the cell below, you should be able to see different augmentations of the same patch. The dot in the images represents the center of the patch, which is the pixel we are going to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (None, None) # Set the size of the patches as a tuple (height, width) \n",
    "\n",
    "img_index = None # choose an image to extract the patch from\n",
    "location = (None, None) # define the location of the patch (y, x) - coordinate\n",
    "\n",
    "patch_extractor = PatchExtractor(patch_size, 30, True)\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (5, 5)\n",
    "plt.imshow(patch_extractor.get_patch(train_data.imgs[img_index], location))\n",
    "plt.scatter(*[p/2 for p in patch_extractor.patch_size], alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a batch creator (15pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to create a BatchCreator. The BatchCreator will allow us to generate batches to train on. These batches contain a set of (class-balanced) samples or patches, and their corresponding labels. The data returned by the ```BatchCreator``` can directly be fed into the neural network for training or classification. \n",
    "\n",
    "Firs we define a usefull function for padding images with a zero-border:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(images, patch_size):\n",
    "    ''' images: list of images (numpy arrays)\n",
    "    returns a padded version of the images, with a border of half the patch_size around each image\n",
    "    '''\n",
    "    half_py, half_px = [p//2 for p in patch_size]\n",
    "    paddings = ((0, 0), (half_py, half_py), (half_px, half_px), (0, 0))\n",
    "    return np.pad(np.array(images), pad_width=paddings, mode='constant') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, implement the ```create_batch``` method in the ```BatchCreator``` class. It should return the patches and labels (```x_data``` and ```y_data```) in the right structure for direct use in Keras (do not confuse the variables (```x,y```) used for coordinates and (```x_data```, ```y_data```) used for images and labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchCreator:\n",
    "    \n",
    "    def __init__(self, patch_extractor, dataset):\n",
    "        self.patch_extractor = patch_extractor\n",
    "        \n",
    "        # the images are padded with half the patch-size around the border\n",
    "        # this way, we don't risk extracting patches from the border, that extend beyond the original image\n",
    "        self.imgs = pad(dataset.imgs, patch_extractor.patch_size)\n",
    "        self.lbls = pad(np.expand_dims(dataset.lbls, 3), patch_extractor.patch_size)\n",
    "        \n",
    "        # pre calculate the positive and negative indices\n",
    "        self.p_idxs = np.where(np.array(dataset.lbls) > 0)\n",
    "        self.n_idxs = np.where((np.array(dataset.msks) > 0) & ~(np.array(dataset.lbls) > 0))\n",
    "\n",
    "    def create_batch(self, batch_size):\n",
    "        '''\n",
    "        returns a class-balanced array of patches (x) with corresponding labels (y) in one-hot structure\n",
    "        '''\n",
    "        # unpack the positive and negative indices\n",
    "        pi, py, px = self.p_idxs\n",
    "        ni, ny, nx = self.n_idxs\n",
    "        \n",
    "        x_data = np.zeros((batch_size, *self.patch_extractor.patch_size, 3))\n",
    "        y_data = np.zeros((batch_size, 1, 1, 2)) # one-hot encoding\n",
    "        \n",
    "        ### Your code starts here\n",
    "        # fill x_data and y_data with the correct data (use both positive and negative examples!):\n",
    "        \n",
    "        # for each i:\n",
    "        #    random_index = np.random.choice(len(pi))\n",
    "        #    img_index = pi[random_index] (or ni[random_index])\n",
    "        #    location = py[random_index], px[random_index] (or ny, nx)\n",
    "        #    x_data[i] = self.patch_extractor.get_patch(self.imgs[img_index], location)\n",
    "        #    y_data[i] = ?\n",
    "        \n",
    "        ### Your code ends here\n",
    "      \n",
    "        return x_data, y_data\n",
    "    \n",
    "    def create_image_batch(self):\n",
    "        '''\n",
    "        returns a single augmented image (x) with corresponding labels (y) in one-hot structure\n",
    "        '''\n",
    "        random_index = np.random.choice(len(self.imgs))\n",
    "        img, lbl = self.imgs[random_index], self.lbls[random_index]\n",
    "        patch_img, patch_lbl = patch_extractor.get_image(img, lbl)\n",
    "        \n",
    "        # adds a sample-dimension\n",
    "        x_data = np.expand_dims(patch_img, 0)\n",
    "\n",
    "        # calculate the dimensions for the label\n",
    "        h, w, _ = patch_lbl.shape\n",
    "        ph, pw = [p//2 for p in self.patch_extractor.patch_size]\n",
    "       \n",
    "        # encodes the label in one hot structure\n",
    "        y_data = np.zeros((1, h - 2 * ph, w - 2 * pw, 2))\n",
    "        y_data[0, :, :, 1][patch_lbl[ph:-ph, pw:-pw, 0]] = 1\n",
    "        y_data[0, :, :, 0][~patch_lbl[ph:-ph, pw:-pw, 0]] = 1\n",
    "        \n",
    "        return x_data, y_data\n",
    "        \n",
    "    def get_generator(self, batch_size):\n",
    "        '''returns a generator that will yield batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_batch(batch_size)\n",
    "            \n",
    "    def get_image_generator(self):\n",
    "        '''returns a generator that will yield image-batches infinitely'''\n",
    "        while True:\n",
    "            yield self.create_image_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our BatchCreator! Half of the patches in the visualization below should represent vessel patches (having a vessel in the center of the patch). The other half should represent background patches (no vessel in the center of the patch). The dot in the images represents the center of the patch, which is the pixel we are going to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_creator = BatchCreator(patch_extractor, train_data)\n",
    "\n",
    "# create a batch\n",
    "x, y = batch_creator.create_batch(28)\n",
    "# visualize it\n",
    "matplotlib.rcParams['figure.figsize'] = (20, 12)\n",
    "f, axes = plt.subplots(4, 7)\n",
    "i = 0;\n",
    "for ax_row in axes:\n",
    "    for ax in ax_row:\n",
    "        ax.imshow(x[i])\n",
    "        ax.set_title('class: {}'.format(np.argmax(y[i, 0, 0])))\n",
    "        ax.scatter(*[p/2 for p in patch_extractor.patch_size], alpha=0.5)\n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the first fully convolutional network (10pts)\n",
    "\n",
    "We will now start with defining our initial network architecture in Keras.\n",
    "What is important to train a fully convolutional neural network on patches, is that **the size of the feature maps within the network goes down to exactly 1x1 for the final feature map**. \n",
    "We will therefore use **valid** convolutions (in contrast to **same** convolutions) and pooling operations to reduce the size of the feature maps.\n",
    "This requires some computation, but luckily it is pretty straightforward:  \n",
    "\n",
    "If we define:\n",
    "```\n",
    "i = Input featuremap size\n",
    "o = Output featuremap size \n",
    "f = Convolution filter size \n",
    "m = Pooling size\n",
    "```\n",
    "Then:\n",
    "\n",
    "Output size of a feature map after convolution with a convolution filter of size f:\n",
    "```python\n",
    "o = i - (f - 1)\n",
    "```\n",
    "Output size of a feature map after pooling of size m:\n",
    "```python\n",
    "o = floor(i/m)\n",
    "```\n",
    "\n",
    "### Model definition\n",
    "Let's define a baseline model:\n",
    "* input layer \n",
    "* 32 filters of 4x4\n",
    "* 32 filters of 3x3\n",
    "* pooling\n",
    "* 64 filters of 3x3\n",
    "* 64 filters of 3x3\n",
    "* pooling\n",
    "* 128 filters of 3x3\n",
    "* 64 filters of 1x1\n",
    "* 2 filters of 1x1 \n",
    "\n",
    "After every convolution we will apply a **relu** non-linearity. The last layer should have a **softmax** non-linearity to transform the 2 filters in probabilities for the vessel and background class.  \n",
    "\n",
    "The reason to start with the less common 4x4 convolutions, is that we wish to define a patch-size that has an odd number of pixels (so that we can define a single center pixel that corresponds with the label). At the same time, we want the input to the pooling layers to be even, so we don't exclude the pixels around the border. The 1x1 convolutions in the deeper layers act as the dense part of a classification network: we no longer take contextual information into account, but only do recombination of features. The final two filters represent the two classes background and vessel.\n",
    "\n",
    "**NOTE**: Since a fully-convolutional network can process input patches of (almost) any size, we should not hard-code a specific size in the ```Input``` layer. The way of doing this is to define the height and the width of input patches as ```None```. The same trick is used to avoid specifying a mini-batch size, which can be changed arbitrarily. We do hard-code the presence of 3 channels, because we are going to process RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DEFINE THE ABOVE DESCRIBED MODEL HERE\n",
    "x_in = Input(batch_shape=(None, None, None, 3)) # DO NOT REPLACE None HERE! It is actually needed!!!\n",
    "\n",
    "### Your code starts here\n",
    "\n",
    "x_out = None # this is the output fo your network, used later on in this notebook\n",
    "\n",
    "### Your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "**Question:**\n",
    "What patch-size do we need to define in order to get an output size of 1x1 for the above network definition?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss and compile the model (5 pts)\n",
    "\n",
    "Although Keras has many built-in cost-functions, we will define our own here as an exercise. \n",
    "It takes as input two tensors: ```y_true``` and ```y_pred```, which have shape ```(samples, height, width, classes)``` and should return a single value representing the loss of this batch. For this assignment we will use the cross-entropy (sometimes also referred to as log-loss). In this case it is equivalent to the mean log of the differences between true label and the predicted label.\n",
    "\n",
    "Note that the inputs (```y_true, y_pred```) are not numpy arrays, but Tensorflow tensor. This means we cannot apply numpy operations on them! Luckily, there is a very similar API we can use in Tensorflow, which we can import through ```keras.backend```. In this assignment, this backend is loaded as ```K```, so you can use ```K.mean, K.sum, K.log``` etc., where you would otherwise use ```np.mean, np.sum, np.log```\n",
    "\n",
    "***Note: *** if you don't succeed in finding a correct implementation, you can change the reference to ```loss_function``` in ```model_1.compile``` to ``` 'categorical_crossentropy' ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1 = Model(x_in, x_out)\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "    # calculate the log-loss \n",
    "    loss = None\n",
    "    \n",
    "    \n",
    "    return loss\n",
    "\n",
    "# define the optimizer. For the initial experiments we will use SGD, later you can try to improve upon this\n",
    "learning_rate = 0.01\n",
    "optimizer = SGD(learning_rate)\n",
    "\n",
    "# uncomment if you wish to skip implementing your own cost-function\n",
    "# loss_function = 'categorical_crossentropy'\n",
    "model_1.compile(optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging:\n",
    "Here we define a logger class, that will store usefull data througout the training procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downscale(images, stride):\n",
    "    # Downscale if the network does pooling\n",
    "    return np.array(images)[:, ::stride, ::stride]\n",
    "\n",
    "def calculate_dice(x, y):\n",
    "    '''returns the dice similarity score, between two boolean arrays'''\n",
    "    return 2 * np.count_nonzero(x & y) / (np.count_nonzero(x) + np.count_nonzero(y))\n",
    "    \n",
    "class Logger(keras.callbacks.Callback):\n",
    "\n",
    "    def __init__(self, validation_data, patch_size, stride=1):\n",
    "        self.val_imgs = pad(validation_data.imgs, patch_size) / 255.\n",
    "        self.val_lbls = downscale(validation_data.lbls, stride) > 0\n",
    "        self.val_msks = downscale(validation_data.msks, stride) > 0\n",
    "         \n",
    "        self.losses = []\n",
    "        self.dices = []\n",
    "        self.best_dice = 0\n",
    "        self.best_model = None\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        dice = self.validate()\n",
    "        self.dices.append([len(self.losses), dice])\n",
    "        if dice > self.best_dice:\n",
    "            self.best_dice = dice\n",
    "            self.best_model = self.model.get_weights()\n",
    "        self.plot()\n",
    "           \n",
    "    def validate(self):\n",
    "        predicted_lbls = self.model.predict(self.val_imgs, batch_size=1)[:,:,:,1]>0.5\n",
    "        x = self.val_lbls[self.val_msks]\n",
    "        y = predicted_lbls[self.val_msks]\n",
    "        return calculate_dice(x, y)\n",
    "    \n",
    "    def plot(self):\n",
    "        clear_output()\n",
    "        N = len(self.losses)\n",
    "        train_loss_plt, = plt.plot(range(0, N), self.losses)\n",
    "        dice_plt, = plt.plot(*np.array(self.dices).T)\n",
    "        plt.legend((train_loss_plt, dice_plt), \n",
    "                   ('training loss', 'validation dice'))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network! (5 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to define some parameters, which are needed to train your network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = (None, None) # should be a tuple of width, height corresponding to your network architecture \n",
    "batch_size = None # pick a proper batch-size (something in the range 32-128 would do)\n",
    "\n",
    "# To let the network converge to a reasonable state, steps_per_epoch * epochs should be around 5000 to 10000\n",
    "steps_per_epoch = None # how many steps per epoch?\n",
    "epochs = None # how many epochs? \n",
    "\n",
    "flipping = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do validation during training, we need to define the downscaling factor (which we call ```stride```) due to pooling of our network. That is: how much lower is the resolution of the output compared to the input?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = None # what is the downscaling factor of your network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use all the classes defined so far to initialize data that will be used for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_extractor = PatchExtractor(patch_size)\n",
    "batch_creator = BatchCreator(patch_extractor, train_data)\n",
    "generator = batch_creator.get_generator(batch_size)\n",
    "logger_1 = Logger(validation_data, patch_size, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_1.fit_generator(generator=generator, \n",
    "                      steps_per_epoch=steps_per_epoch, \n",
    "                      epochs=epochs, \n",
    "                      callbacks=[logger_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check results on the validation set\n",
    "Now let's apply the trained model to the validation data to get an idea how well the network is performing. We will define the function ```process_basic``` below to apply the model to a dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_basic(model, dataset, stride=1):\n",
    "    \n",
    "    # pad the original images, so we don't loose the borders in our output\n",
    "    imgs = pad(dataset.imgs, patch_size) / 255.\n",
    "    \n",
    "    # downscale the label and masks if needed (to get the same resolution as the output)\n",
    "    lbls = downscale(dataset.lbls, stride) > 0\n",
    "    msks = downscale(dataset.msks, stride) > 0\n",
    "    \n",
    "    # apply our model to the images\n",
    "    output = model.predict(imgs, batch_size=1)[:,:,:,1]\n",
    "\n",
    "    return lbls, msks, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following function to visually inspect the results and calculate a dice-score for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results(imgs, lbls, msks, output, threshold=0.5):\n",
    "\n",
    "    dices = []\n",
    "    for i, (img, lbl, msk, raw_output) in enumerate(zip(imgs, lbls, msks, output)):\n",
    "        \n",
    "        final_output = raw_output > threshold\n",
    "        \n",
    "        dice = calculate_dice(final_output[msk], lbl[msk])\n",
    "        dices.append(dice)\n",
    "        print('image:', i, 'dice', dice)\n",
    "        \n",
    "        # plot the results\n",
    "        matplotlib.rcParams['figure.figsize'] = (15, 6)\n",
    "        f, axes = plt.subplots(1, 4)\n",
    "        for ax, im, t in zip(axes, \n",
    "                             (img, raw_output, final_output, lbl), \n",
    "                             ('RGB image', 'Soft prediction', 'Thresholded', 'Ground truth')):\n",
    "            ax.imshow(im, cmap='gray')\n",
    "            ax.set_title(t)\n",
    "        plt.show()\n",
    "        \n",
    "    print('mean dice', np.mean(dices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the best model (based on the validation dice score during training), and check the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.set_weights(logger_1.best_model)\n",
    "lbls, msks, output =  process_basic(model_1, validation_data, stride=stride)\n",
    "check_results(validation_data.imgs, lbls, msks, output, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift and stitch\n",
    "\n",
    "You can see that the label image predicted by our network has a **much lower resolution** then the input image.\n",
    "This is due to the pooling operations that we used in our network. \n",
    "A trick to get full resolution output back is to use shift and stitch, where we apply the network multiple times to the same input image, but shifted by a pixel every time. The output images are then stitched back together to obtain a full resolution image.\n",
    "\n",
    "Recent advances have made this technique more or less obsolete and implementation can be cumbersome, but we will provide an implementation of the ```shift_and_stitch``` function for you, that you can use to obtain a full resolution output image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_and_stitch(model, x, patch_size, stride, n_classes):\n",
    "    pad_h, pad_w = [p//2 for p in patch_size]\n",
    "    stride_h, stride_w = stride\n",
    "    n_batches, height, width, n_channels = x.shape\n",
    "\n",
    "    target_shape = n_batches, height + 2 * pad_h, width + 2 * pad_w, n_channels\n",
    "    y = np.zeros((n_batches, height + stride_h, width + stride_h, n_classes))\n",
    "    for r in range(stride_h):\n",
    "        for c in range(stride_w):\n",
    "            x_in = np.zeros(target_shape)\n",
    "            h_start, w_start = pad_h - r, pad_w  - c\n",
    "            x_in[:, h_start:h_start + height, w_start: w_start + width] = x\n",
    "            y_out = model.predict(x_in, batch_size=1)\n",
    "            _, h, w, _ = y_out.shape\n",
    "            y[:, r:h*stride_h:stride_h, c:w*stride_w:stride_w,:] = y_out\n",
    "    return y[:,:height,:width,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shift_and_stitch(model, dataset, patch_size, stride):\n",
    "    imgs = np.array(dataset.imgs) / 255.\n",
    "    lbls = np.array(dataset.lbls) > 0\n",
    "    msks = np.array(dataset.msks) > 0\n",
    "        \n",
    "    output = shift_and_stitch(model, imgs, patch_size, (stride, stride), 2)\n",
    "    return lbls, msks, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls, msks, output = process_shift_and_stitch(model_1, validation_data, patch_size, stride)\n",
    "check_results(validation_data.imgs, lbls, msks, output, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get a full resolution output map, which looks much smoother already!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "**Question:**\n",
    "Looking at the images above, the initial threshold of 0.5 is probably not ideal for optimal performance. Would you need a higher or a lower threshold? What could be a reason for this? (hint: think of the way we present data during training)\n",
    "</font color=\"blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the test set and submit to grand-challenge.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can process images in the test set and submit your results to grand-challenge.\n",
    "You can use the next cell to process images with shift and stitch.\n",
    "You can tune the threshold and (optionally) apply the mask before submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define output folder\n",
    "result_output_folder = './results'\n",
    "if not os.path.exists(result_output_folder):\n",
    "    os.makedirs(result_output_folder)\n",
    "\n",
    "# extract padding information from patch size\n",
    "pad_h, pad_w = [p//2 for p in patch_size]\n",
    "\n",
    "# threshold to select segmented pixels\n",
    "threshold = 0.5\n",
    "\n",
    "# pick the best model\n",
    "model_1.set_weights(logger_1.best_model)\n",
    "\n",
    "# pad all images in the test set\n",
    "imgs = pad(test_imgs, patch_size) / 255.\n",
    "\n",
    "# apply shift and stitch\n",
    "output = shift_and_stitch(model_1, imgs, patch_size, (stride, stride), 2)\n",
    "\n",
    "# save all output masks as png images\n",
    "for i in range(output.shape[0]):\n",
    "\n",
    "    # output has to be:\n",
    "    # - cropped, because of previous padding\n",
    "    # - thresholded\n",
    "    # - converted to grayscale\n",
    "    segmentation = 255 * (output[i, pad_h:-pad_h, pad_w:-pad_w].squeeze() > threshold).astype(int)\n",
    "    \n",
    "    # save to disk\n",
    "    im = Image.fromarray(segmentation.astype('uint8'))\n",
    "    im.save(os.path.join(result_output_folder, \"{}.png\".format(i + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive('results', 'zip', result_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now download this zipfile with this link: [results.zip](results.zip).  \n",
    "Next, upload your result to the challenge website (https://drive.grand-challenge.org/evaluation/submissions/create/) and see how well you performed compared to your fellow students! You can submit as often as you want, only the best result counts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network 2: removing pooling layers (10 pts)\n",
    "The pooling layers decrease the resolution of our ouput image, and shift-and-stich can only partly recover this for us. Instead, let's explore what happens if we completely remove the pooling layers. Because we want the network to work on odd-sized patches again, we will now set the size of the first convolutional filter back to the normal 3x3 value.\n",
    "\n",
    "Implement the following network:\n",
    "* input layer \n",
    "* 32 filters of 3x3\n",
    "* 32 filters of 3x3\n",
    "* 64 filters of 3x3\n",
    "* 64 filters of 3x3\n",
    "* 128 filters of 3x3\n",
    "* 64 filters of 1x1\n",
    "* 2 filters of 1x1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = Input(batch_shape=(None, None, None, 3))\n",
    "x_out = None\n",
    "\n",
    "model_2 = Model(x_in, x_out)\n",
    "model_2.compile(optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "**Question:**\n",
    "What is the new patch size of the network without the pooling layers?\n",
    "</font color=\"blue\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the new patch_size and stride for this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = None # tuple of width, height\n",
    "stride = None # downscaling factor due to pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_extractor = PatchExtractor(patch_size)\n",
    "batch_creator = BatchCreator(patch_extractor, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the new patches we will be training on: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = batch_creator.create_batch(24)\n",
    "f, axes = plt.subplots(4, 6)\n",
    "i = 0;\n",
    "for ax_row in axes:\n",
    "    for ax in ax_row:\n",
    "        ax.imshow(x[i])\n",
    "        ax.scatter(*[p/2 for p in patch_extractor.patch_size], alpha=0.5)\n",
    "        i += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the new network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generator = batch_creator.get_generator(batch_size)\n",
    "\n",
    "logger_2 = Logger(validation_data, patch_size, stride=stride)\n",
    "\n",
    "model_2.fit_generator(generator=generator, \n",
    "                      steps_per_epoch=steps_per_epoch, \n",
    "                      epochs=epochs, \n",
    "                      callbacks=[logger_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the performance of the new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new model does not reduce the resolution of the output image, so it can be applied directly to new images!\n",
    "Let's see how well this model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.set_weights(logger_2.best_model)\n",
    "\n",
    "lbls, msks, output =  process_basic(model_2, validation_data)\n",
    "check_results(validation_data.imgs, lbls, msks, output, threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to grand-challenge.org (again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define output folder\n",
    "result_output_folder = './results2'\n",
    "if not os.path.exists(result_output_folder):\n",
    "    os.makedirs(result_output_folder)\n",
    "\n",
    "# threshold to select segmented pixels\n",
    "threshold = 0.5\n",
    "\n",
    "# pick the best model\n",
    "model_2.set_weights(logger_2.best_model)\n",
    "\n",
    "# pad all images in the test set\n",
    "imgs = pad(test_imgs, patch_size) / 255.\n",
    "   \n",
    "# apply our model to the images\n",
    "output = model_2.predict(imgs, batch_size=1)[:,:,:,1]\n",
    "\n",
    "# save all output masks as png images\n",
    "for i in range(output.shape[0]):\n",
    "\n",
    "    # output has to be:\n",
    "    # - thresholded\n",
    "    # - converted to grayscale\n",
    "    segmentation = 255 * (output[i, :, :].squeeze() > threshold).astype(int)\n",
    "    \n",
    "    # save to disk\n",
    "    im = Image.fromarray(segmentation.astype('uint8'))\n",
    "    im.save(os.path.join(result_output_folder, \"{}.png\".format(i + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.make_archive('results2', 'zip', result_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now download this zipfile with this link: [results2.zip](results.zip).  \n",
    "Next, upload your result to the challenge website (https://drive.grand-challenge.org/evaluation/submissions/create/) and see how well you performed compared to your fellow students! You can submit as often as you want, only the best result counts.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network 3: dilated convolutions (10 pts)\n",
    "<img align=\"right\" width=\"300\" src=\"images/dilation.gif\">\n",
    "\n",
    "You will notice that the performance of the last network for this task is still reasonable, even if we train on smaller patches. However, to get state-of-the-art results, it is sometimes necessary to include a larger context. If we wish to do this without introducing pooling operations or increasing the complexity of the network too much, we have another option: **dilated convolutions**.\n",
    "\n",
    "Dilated convolutions work like regular convolutions, but introduce a spacing between the parameters of the filter. This way, they expand the area covered by the filter (often referred to as the **receptive field**). Unlike pooling operations, they do not reduce the resolution of the feature maps. Let's experiment with them, and define a new network architecture that includes dilated convolutions. See the image to the right for an example of a 3x3 convolution filter with dilation rate 2 (image taken from https://github.com/vdumoulin/conv_arithmetic).\n",
    "\n",
    "Implement the following network\n",
    "* input layer \n",
    "* 32 filters of 3x3\n",
    "* 32 filters of 3x3\n",
    "* 64 filters of 3x3, dilation rate 2\n",
    "* 64 filters of 3x3, dilation rate 4\n",
    "* 128 filters of 3x3, dilation rate 8\n",
    "* 64 filters of 1x1\n",
    "* 2 filters of 1x1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = Input(batch_shape=(None, None, None, 3))\n",
    "x_out = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Model(x_in, x_out)\n",
    "model_3.compile(optimizer, loss_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"blue\">\n",
    "**Question:**\n",
    "    \n",
    "What patch-size do we need to define in order to get an output size of 1x1 for the above network definition?\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the new patch_size and stride for this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = None \n",
    "stride = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_extractor = PatchExtractor(patch_size)\n",
    "batch_creator = BatchCreator(patch_extractor, train_data)\n",
    "patch_generator = batch_creator.get_generator(batch_size)\n",
    "\n",
    "logger_3 = Logger(validation_data, patch_size, stride=stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.fit_generator(generator=patch_generator, \n",
    "                      steps_per_epoch=steps_per_epoch, \n",
    "                      epochs=epochs, \n",
    "                      callbacks=[logger_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will inspect the results on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls, msks, output = process_basic(model_3, validation_data)\n",
    "check_results(validation_data.imgs, lbls, msks, output, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to grand-challenge (yes, again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-use previous code to process images segmented with the dilated network \n",
    "# and make a new submission to grand-challenge to check the performance on the test set\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on full images\n",
    "The results with the dilated network don't look as good as the previous results yet. \n",
    "One reason for this could be that the network needs to aggregate more input data (larger patches) to a single label, and therefore may need longer training time. Let's see if we can do that more efficiently.\n",
    "\n",
    "Instead of training on patches, we can actually use the entire image as input to the network, and apply the backpropagation over all pixels at the same time. This allows us to supply much more data to the network at a time. Let's try to fine-tune the dilated model by training for some more epochs on full images.\n",
    "\n",
    "First, we will define the new generator and visualize the input to the network. Note the different shapes of the input image and label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = batch_creator.get_image_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell repeatedly to see different augmented images\n",
    "im, label = next(image_generator)\n",
    "print('input shape:', im.shape, 'output shape:', label.shape)\n",
    "matplotlib.rcParams['figure.figsize'] = (15, 5)\n",
    "f, (ax0, ax1) = plt.subplots(1, 2)\n",
    "ax0.imshow(im[0])\n",
    "ax1.imshow(label[0,:,:,1], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the cell below will continue training the model, this time on full images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_3.fit_generator(generator=image_generator, \n",
    "                      steps_per_epoch=10, \n",
    "                      epochs=100, \n",
    "                      callbacks=[logger_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls, msks, output =  process_basic(model_3, validation_data)\n",
    "check_results(validation_data.imgs, lbls, msks, output, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit to grand-challenge.org (...and again...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once again, re-use previous code to process images with this fine-tuned network\n",
    "# and make a submission to grand-challenge.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to improve the results! (30 pts)\n",
    "It is now up to you to pick a network architecture and a training strategy and try to get the best performance!\n",
    "\n",
    "Here is an incomplete list of things you could experiment with:\n",
    "\n",
    "- **Network architecture**: so far we've implemented 3 different network architecture, each with its pro's and cons. There are many other things you can try. You can play with the number of filters, the number of layers, adding drop-out or batch-normalization etc.\n",
    "\n",
    "- **Training strategy**: we've implemented 2 different training strategies: patch-based or on full images. Maybe there are other options (alternating between the two, or selecting patches at hard locations to put more attention to the places where the network still fails).\n",
    "\n",
    "- **Optimizer**: we've used SGD so far, maybe another optimizer would improve your results. Also try to experiment with the learning rate, or a apply a decay in the learning rate.\n",
    "\n",
    "- **Augmentations**: we've only applied horizontal flipping to the patches and rotation to full images. Other useful augmentations may include non-linear deformations or color augmentation such as gamma-corrections.\n",
    "\n",
    "**NOTE:** Make sure your notebook can be executed cell by cell from top to bottom, so don't change the cells above, but instead add new cells below demonstrating your experiments. Please document clearly what experiments you've performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your experiments here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
